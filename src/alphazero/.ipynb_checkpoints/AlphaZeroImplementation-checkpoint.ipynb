{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a2b6a16",
   "metadata": {},
   "source": [
    "# Implementation of Alpha Zero for games TicTacToe and Connect 4\n",
    "This follows the ideas of idea.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d090f5e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#import torch\n",
    "#import torch.nn as nn\n",
    "#import torch.nn.functional as F\n",
    "#from tqdm.notebook import trange\n",
    "\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57524aa1",
   "metadata": {},
   "source": [
    "### Games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a097e1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToe:\n",
    "    def __init__(self):\n",
    "        self.row_count = 3\n",
    "        self.column_count = 3\n",
    "        self.action_size = self.row_count * self.column_count\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return \"TicTacToe\"\n",
    "        \n",
    "    def get_initial_state(self):\n",
    "        return np.zeros((self.row_count, self.column_count))\n",
    "    \n",
    "    def get_next_state(self, state, action, player):\n",
    "        row = action // self.column_count\n",
    "        column = action % self.column_count\n",
    "        state[row, column] = player\n",
    "        return state\n",
    "    \n",
    "    def get_valid_moves(self, state):\n",
    "        return (state.reshape(-1) == 0).astype(np.uint8)\n",
    "    \n",
    "    def check_win(self, state, action):\n",
    "        if action == None:\n",
    "            return False\n",
    "        \n",
    "        row = action // self.column_count\n",
    "        column = action % self.column_count\n",
    "        player = state[row, column]\n",
    "        \n",
    "        return (\n",
    "            np.sum(state[row, :]) == player * self.column_count\n",
    "            or np.sum(state[:, column]) == player * self.row_count\n",
    "            or np.sum(np.diag(state)) == player * self.row_count\n",
    "            or np.sum(np.diag(np.flip(state, axis=0))) == player * self.row_count\n",
    "        )\n",
    "    \n",
    "    def get_value_and_terminated(self, state, action):\n",
    "        if self.check_win(state, action):\n",
    "            return 1, True\n",
    "        if np.sum(self.get_valid_moves(state)) == 0:\n",
    "            return 0, True\n",
    "        return 0, False\n",
    "    \n",
    "    def get_opponent(self, player):\n",
    "        return -player\n",
    "    \n",
    "    def get_opponent_value(self, value):\n",
    "        return -value\n",
    "    \n",
    "    def change_perspective(self, state, player):\n",
    "        return state * player\n",
    "    \n",
    "    def get_encoded_state(self, state):\n",
    "        encoded_state = np.stack(\n",
    "            (state == -1, state == 0, state == 1)\n",
    "        ).astype(np.float32)\n",
    "        \n",
    "        if len(state.shape) == 3:\n",
    "            encoded_state = np.swapaxes(encoded_state, 0, 1)\n",
    "        \n",
    "        return encoded_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "682c4ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConnectFour:\n",
    "    def __init__(self):\n",
    "        self.row_count = 6\n",
    "        self.column_count = 7\n",
    "        self.action_size = self.column_count\n",
    "        self.in_a_row = 4\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return \"ConnectFour\"\n",
    "        \n",
    "    def get_initial_state(self):\n",
    "        return np.zeros((self.row_count, self.column_count))\n",
    "    \n",
    "    def get_next_state(self, state, action, player):\n",
    "        row = np.max(np.where(state[:, action] == 0))\n",
    "        state[row, action] = player\n",
    "        return state\n",
    "    \n",
    "    def get_valid_moves(self, state):\n",
    "        return (state[0] == 0).astype(np.uint8)\n",
    "    \n",
    "    def check_win(self, state, action):\n",
    "        if action == None:\n",
    "            return False\n",
    "        \n",
    "        row = np.min(np.where(state[:, action] != 0))\n",
    "        column = action\n",
    "        player = state[row][column]\n",
    "\n",
    "        def count(offset_row, offset_column):\n",
    "            for i in range(1, self.in_a_row):\n",
    "                r = row + offset_row * i\n",
    "                c = action + offset_column * i\n",
    "                if (\n",
    "                    r < 0 \n",
    "                    or r >= self.row_count\n",
    "                    or c < 0 \n",
    "                    or c >= self.column_count\n",
    "                    or state[r][c] != player\n",
    "                ):\n",
    "                    return i - 1\n",
    "            return self.in_a_row - 1\n",
    "\n",
    "        return (\n",
    "            count(1, 0) >= self.in_a_row - 1 # vertical\n",
    "            or (count(0, 1) + count(0, -1)) >= self.in_a_row - 1 # horizontal\n",
    "            or (count(1, 1) + count(-1, -1)) >= self.in_a_row - 1 # top left diagonal\n",
    "            or (count(1, -1) + count(-1, 1)) >= self.in_a_row - 1 # top right diagonal\n",
    "        )\n",
    "    \n",
    "    def get_value_and_terminated(self, state, action):\n",
    "        if self.check_win(state, action):\n",
    "            return 1, True\n",
    "        if np.sum(self.get_valid_moves(state)) == 0:\n",
    "            return 0, True\n",
    "        return 0, False\n",
    "    \n",
    "    def get_opponent(self, player):\n",
    "        return -player\n",
    "    \n",
    "    def get_opponent_value(self, value):\n",
    "        return -value\n",
    "    \n",
    "    def change_perspective(self, state, player):\n",
    "        return state * player\n",
    "    \n",
    "    def get_encoded_state(self, state):\n",
    "        encoded_state = np.stack(\n",
    "            (state == -1, state == 0, state == 1)\n",
    "        ).astype(np.float32)\n",
    "        \n",
    "        if len(state.shape) == 3:\n",
    "            encoded_state = np.swapaxes(encoded_state, 0, 1)\n",
    "        \n",
    "        return encoded_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1043ac",
   "metadata": {},
   "source": [
    "### OUR GAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29880fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import StrEnum, Flag, auto\n",
    "from functools import reduce\n",
    "\n",
    "class Command(StrEnum):\n",
    "    INFO = \"info\"\n",
    "    HELP = \"help\"\n",
    "    OPTIONS = \"options\"\n",
    "    NEWGAME = \"newgame\"\n",
    "    VALIDMOVES = \"validmoves\"\n",
    "    BESTMOVE = \"bestmove\"\n",
    "    PLAY = \"play\"\n",
    "    PASS = \"pass\"\n",
    "    UNDO = \"undo\"\n",
    "    EXIT = \"exit\"\n",
    "    GET = \"get\"\n",
    "    SET = \"set\"\n",
    "\n",
    "class PlayerColor():\n",
    "    WHITE = 1\n",
    "    BLACK = -1\n",
    "\n",
    "class GameType(Flag):\n",
    "  Base = auto()\n",
    "  M = auto()\n",
    "  L = auto()\n",
    "  P = auto()\n",
    "\n",
    "  @classmethod\n",
    "  def parse(cls, type: str):\n",
    "    if type:\n",
    "      base, *expansions = type.split(\"+\")\n",
    "      try:\n",
    "        if GameType[base] != GameType.Base or expansions == [\"\"] or len(expansions) > 1 and type.find(\"+\") >= 0: raise KeyError()\n",
    "        return reduce(lambda type, expansion: type | expansion, [GameType[expansion] for expansion in (expansions[0] if expansions else \"\")], GameType[base])\n",
    "      except KeyError:\n",
    "        raise ValueError(f\"'{type}' is not a valid GameType\")\n",
    "    return GameType.Base\n",
    "\n",
    "  def __str__(self) -> str:\n",
    "    return \"\".join(str(gametype.name) + (\"+\" if gametype is GameType.Base and len(self) > 1 else \"\") for gametype in self)\n",
    "\n",
    "class BugType(StrEnum):\n",
    "  QUEEN_BEE = \"Q\"\n",
    "  SPIDER = \"S\"\n",
    "  BEETLE = \"B\"\n",
    "  GRASSHOPPER = \"G\"\n",
    "  SOLDIER_ANT = \"A\"\n",
    "  MOSQUITO = \"M\"\n",
    "  LADYBUG = \"L\"\n",
    "  PILLBUG = \"P\"\n",
    "\n",
    "class Direction(StrEnum):\n",
    "  RIGHT = \"|-\"\n",
    "  UP_RIGHT = \"|/\"\n",
    "  UP_LEFT = \"\\\\|\"\n",
    "  LEFT = \"-|\"\n",
    "  DOWN_LEFT = \"/|\"\n",
    "  DOWN_RIGHT = \"|\\\\\"\n",
    "  BELOW = \"\"\n",
    "  ABOVE = \"|\"\n",
    "\n",
    "  @classmethod\n",
    "  def flat(cls):\n",
    "    return [direction for direction in Direction if direction is not Direction.ABOVE and direction is not Direction.BELOW]\n",
    "  \n",
    "  @classmethod\n",
    "  def flat_left(cls):\n",
    "    return [direction for direction in Direction if direction.is_left]\n",
    "\n",
    "  @classmethod\n",
    "  def flat_right(cls):\n",
    "    return [direction for direction in Direction if direction.is_right]\n",
    "\n",
    "  def __str__(self) -> str:\n",
    "    return self.replace(\"|\", \"\")\n",
    "\n",
    "  @property\n",
    "  def opposite(self):\n",
    "    match self:\n",
    "      case Direction.BELOW | Direction.ABOVE:\n",
    "        return list(Direction)[(self.delta_index - 5) % 2 + 6]\n",
    "      case _:\n",
    "        return list(Direction)[(self.delta_index + 3) % 6]\n",
    "\n",
    "  @property\n",
    "  def left_of(self):\n",
    "    match self:\n",
    "      case Direction.BELOW | Direction.ABOVE:\n",
    "        return self\n",
    "      case _:\n",
    "        return list(Direction)[(self.delta_index + 1) % 6]\n",
    "  \n",
    "  @property\n",
    "  def right_of(self):\n",
    "    match self:\n",
    "      case Direction.BELOW | Direction.ABOVE:\n",
    "        return self\n",
    "      case _:\n",
    "        return list(Direction)[(self.delta_index + 5) % 6]\n",
    "\n",
    "  @property\n",
    "  def delta_index(self) -> int:\n",
    "    return list(Direction).index(self)\n",
    "\n",
    "  @property\n",
    "  def is_right(self) -> bool:\n",
    "    return self is Direction.RIGHT or self is Direction.UP_RIGHT or self is Direction.DOWN_RIGHT\n",
    "\n",
    "  @property\n",
    "  def is_left(self) -> bool:\n",
    "    return self is Direction.LEFT or self is Direction.UP_LEFT or self is Direction.DOWN_LEFT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d457d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Final, Optional\n",
    "import re\n",
    "\n",
    "class Position():\n",
    "  def __init__(self, q: int, r: int):\n",
    "    self.row: Final[int] = q\n",
    "    self.col: Final[int] = r\n",
    "\n",
    "  def __str__(self) -> str:\n",
    "    return f\"({self.row}, {self.col})\"\n",
    "\n",
    "  def __hash__(self) -> int:\n",
    "    return hash((self.row, self.col))\n",
    "\n",
    "  def __eq__(self, value: object) -> bool:\n",
    "    return self is value or isinstance(value, Position) and self.row == value.row and self.col == value.col\n",
    "\n",
    "  def __add__(self, other: object):\n",
    "    return Position(self.row + other.row, self.col + other.col) if isinstance(other, Position) else NotImplemented\n",
    "    \n",
    "  def __sub__(self, other: object):\n",
    "    return Position(self.row - other.row, self.col - other.col) if isinstance(other, Position) else NotImplemented\n",
    "\n",
    "class Bug():\n",
    "\n",
    "  def __init__(self, color: int, bug_type: BugType, bug_id: int = 0) -> None:\n",
    "    self.color: Final[PlayerColor] = color\n",
    "    self.type: Final[BugType] = bug_type\n",
    "    self.num: Final[int] = bug_id\n",
    "\n",
    "  def __hash__(self) -> int:\n",
    "    return hash(str(self)[1:])*self.color # TODO: make them have a number that makes sense, like the one in C++\n",
    "  \n",
    "  def __eq__(self, value: object) -> bool:\n",
    "    return self is value or isinstance(value, Bug) and self.color is value.color and self.type is value.type and self.num == value.num\n",
    "\n",
    "class Move():\n",
    "  PASS: Final[str] = \"pass\"\n",
    "    \n",
    "  def __init__(self, bug: Bug, origin: Optional[Position], destination: Position) -> None:\n",
    "    self.bug: Final[Bug] = bug\n",
    "    self.origin: Final[Optional[Position]] = origin\n",
    "    self.destination: Final[Position] = destination\n",
    "\n",
    "  def __hash__(self) -> int:\n",
    "    return hash((self.bug, self.origin, self.destination))\n",
    "\n",
    "  def __eq__(self, value: object) -> bool:\n",
    "    return self is value or isinstance(value, Move) and self.bug == value.bug and self.origin == value.origin and self.destination == value.destination\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "393dedbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Final, Optional, Set\n",
    "\n",
    "class HiveBoard(): # This is simply the implementation of the board\n",
    "  NEIGHBOR_DELTAS: Final[tuple[Position, Position, Position, Position, Position, Position, Position, Position]] = (\n",
    "    Position(1, 0), # Right\n",
    "    Position(1, -1), # Up right\n",
    "    Position(0, -1), # Up left\n",
    "    Position(-1, 0), # Left\n",
    "    Position(-1, 1), # Down left\n",
    "    Position(0, 1), # Down right\n",
    "    Position(0, 0), # Below (no change)\n",
    "    Position(0, 0), # Above (no change)\n",
    "  )\n",
    "\n",
    "  def __init__(self, gamestring: str = \"\") -> None:\n",
    "    #self.type: Final[GameType] = self._parse_gamestring(gamestring) TODO: we learn only on MLP for now\n",
    "    self.player: int = PlayerColor.WHITE\n",
    "    self.turn = 1\n",
    "    self.row_count = 28\n",
    "    self.column_count = 28\n",
    "    self.height_count = 5\n",
    "    self.last_moved : Bug = None\n",
    "    self.board = np.zeros((self.row_count, self.column_count, self.height_count), dtype=int)\n",
    "    self._bug_to_pos: dict[Bug, Optional[Position]] = {}\n",
    "\n",
    "    for color in PlayerColor:\n",
    "      #for expansion in self.type:\n",
    "       # if expansion is GameType.Base:\n",
    "          self._bug_to_pos[Bug(color, BugType.QUEEN_BEE)] = None\n",
    "          # Add ids greater than 0 only for bugs with multiple copies.\n",
    "          for i in range(1, 3):\n",
    "            self._bug_to_pos[Bug(color, BugType.SPIDER, i)] = None\n",
    "            self._bug_to_pos[Bug(color, BugType.BEETLE, i)] = None\n",
    "            self._bug_to_pos[Bug(color, BugType.GRASSHOPPER, i)] = None\n",
    "            self._bug_to_pos[Bug(color, BugType.SOLDIER_ANT, i)] = None\n",
    "          self._bug_to_pos[Bug(color, BugType.GRASSHOPPER, 3)] = None\n",
    "          self._bug_to_pos[Bug(color, BugType.SOLDIER_ANT, 3)] = None\n",
    "      #else:\n",
    "          for expansion in ['M', 'L', 'P']:\n",
    "              self._bug_to_pos[Bug(color, BugType(expansion.name))] = None\n",
    "\n",
    "  @property\n",
    "  def current_player_queen_in_play(self) -> bool:\n",
    "    return bool(self._bug_to_pos[Bug(self.player, BugType.QUEEN_BEE)])\n",
    "\n",
    "  def get_next_state(self, move: Move):\n",
    "      turn += 1\n",
    "      self.player = -self.player\n",
    "      if not move.origin:\n",
    "        self._bug_to_pos[move.bug] = move.destination\n",
    "      else:\n",
    "        self.board[move.origin.r][move.origin.q] = 0 #TODO: how to manage a piece above another one: we should have a 3d image\n",
    "      self.board[move.destination.row][move.destination.column] = move.bug.to_id()\n",
    "\n",
    "\n",
    "  def get_value_and_terminated(self):\n",
    "    black_queen_surrounded = (queen_pos := self._bug_to_pos[Bug(PlayerColor.BLACK, BugType.QUEEN_BEE)]) and all(self._bugs_from_pos(self._get_neighbor(queen_pos, direction)) for direction in Direction.flat())\n",
    "    white_queen_surrounded = (queen_pos := self._bug_to_pos[Bug(PlayerColor.WHITE, BugType.QUEEN_BEE)]) and all(self._bugs_from_pos(self._get_neighbor(queen_pos, direction)) for direction in Direction.flat())\n",
    "    if black_queen_surrounded and white_queen_surrounded:\n",
    "      return 0, True\n",
    "    elif black_queen_surrounded:\n",
    "      return 1, True\n",
    "    elif white_queen_surrounded:\n",
    "      return -1, True\n",
    "    return 0, False\n",
    "\n",
    "  def get_valid_moves(self) -> Set[Move]:\n",
    "      # TODO: da qui\n",
    "      self._valid_moves_cache = []\n",
    "      for bug, pos in self._bug_to_pos.items():\n",
    "        # Iterate over available pieces of the current player\n",
    "        if bug.color is self.player:\n",
    "          # Turn 1 is White player's first turn\n",
    "          if self.turn == 1:\n",
    "            # Can't place the queen on the first turn\n",
    "            if bug.type is not BugType.QUEEN_BEE and self._can_bug_be_played(bug):\n",
    "              # Add the only valid placement for the current bug piece\n",
    "              self._valid_moves_cache.add(Move(bug, None, self.ORIGIN))\n",
    "          # Turn 0 is Black player's first turn\n",
    "          elif self.turn == 2:\n",
    "            # Can't place the queen on the first turn\n",
    "            if bug.type is not BugType.QUEEN_BEE and self._can_bug_be_played(bug):\n",
    "              # Add all valid placements for the current bug piece (can be placed only around the first White player's first piece)\n",
    "              self._valid_moves_cache.update(Move(bug, None, self._get_neighbor(self.ORIGIN, direction)) for direction in Direction.flat())\n",
    "          # Bug piece has not been played yet\n",
    "          elif not pos:\n",
    "            # Check hand placement, and turn and queen placement, related rule.\n",
    "            if self._can_bug_be_played(bug) and (self.current_player_turn != 4 or (self.current_player_turn == 4 and (self.current_player_queen_in_play or (not self.current_player_queen_in_play and bug.type is BugType.QUEEN_BEE)))):\n",
    "              # Add all valid placements for the current bug piece\n",
    "              self._valid_moves_cache.update(Move(bug, None, placement) for placement in self._get_valid_placements())\n",
    "          # A bug piece in play can move only if it's at the top and its queen is in play and has not been moved in the previous player's turn\n",
    "          elif self.current_player_queen_in_play and self._bugs_from_pos(pos)[-1] == bug and self._was_not_last_moved(bug):\n",
    "            # Can't move pieces that would break the hive. Pieces stacked upon other can never break the hive by moving\n",
    "            if len(self._bugs_from_pos(pos)) > 1 or self._can_move_without_breaking_hive(pos):\n",
    "              match bug.type:\n",
    "                case BugType.QUEEN_BEE:\n",
    "                  self._valid_moves_cache.update(self._get_sliding_moves(bug, pos, 1))\n",
    "                case BugType.SPIDER:\n",
    "                  self._valid_moves_cache.update(self._get_sliding_moves(bug, pos, 3))\n",
    "                case BugType.BEETLE:\n",
    "                  self._valid_moves_cache.update(self._get_beetle_moves(bug, pos))\n",
    "                case BugType.GRASSHOPPER:\n",
    "                  self._valid_moves_cache.update(self._get_grasshopper_moves(bug, pos))\n",
    "                case BugType.SOLDIER_ANT:\n",
    "                  self._valid_moves_cache.update(self._get_sliding_moves(bug, pos))\n",
    "                case BugType.MOSQUITO:\n",
    "                  self._valid_moves_cache.update(self._get_mosquito_moves(bug, pos))\n",
    "                case BugType.LADYBUG:\n",
    "                  self._valid_moves_cache.update(self._get_ladybug_moves(bug, pos))\n",
    "                case BugType.PILLBUG:\n",
    "                  self._valid_moves_cache.update(self._get_sliding_moves(bug, pos, 1))\n",
    "                  self._valid_moves_cache.update(self._get_pillbug_special_moves(pos))\n",
    "            else:\n",
    "              match bug.type:\n",
    "                case BugType.MOSQUITO:\n",
    "                  self._valid_moves_cache.update(self._get_mosquito_moves(bug, pos, True))\n",
    "                case BugType.PILLBUG:\n",
    "                  self._valid_moves_cache.update(self._get_pillbug_special_moves(pos))\n",
    "                case _:\n",
    "                  pass\n",
    "      return self._valid_moves_cache\n",
    "\n",
    "  def _get_valid_placements(self) -> Set[Position]:\n",
    "    placements: Set[Position] = set()\n",
    "    # Iterate over all placed bug pieces of the current player\n",
    "    for bug, pos in self._bug_to_pos.items():\n",
    "      if bug.color is self.current_player_color and pos and self._is_bug_on_top(bug):\n",
    "        # Iterate over all neighbors of the current bug piece\n",
    "        for direction in Direction.flat():\n",
    "          neighbor = self._get_neighbor(pos, direction)\n",
    "          # If the neighboring tile is empty\n",
    "          if not self._bugs_from_pos(neighbor):\n",
    "            # If all neighbor's neighbors are empty or of the same color, add the neighbor as a valid placement\n",
    "            if all(not self._bugs_from_pos(self._get_neighbor(neighbor, dir)) or self._bugs_from_pos(self._get_neighbor(neighbor, dir))[-1].color is self.current_player_color for dir in Direction.flat() if dir is not direction.opposite):\n",
    "              placements.add(neighbor)\n",
    "    return placements\n",
    "\n",
    "  def _get_sliding_moves(self, bug: Bug, origin: Position, depth: int = 0) -> Set[Move]:\n",
    "    destinations: Set[Position] = set()\n",
    "    visited: Set[Position] = set()\n",
    "    stack: Set[tuple[Position, int]] = {(origin, 0)}\n",
    "    unlimited_depth = depth == 0\n",
    "    while stack:\n",
    "      current, current_depth = stack.pop()\n",
    "      visited.add(current)\n",
    "      if unlimited_depth or current_depth == depth:\n",
    "        destinations.add(current)\n",
    "      if unlimited_depth or current_depth < depth:\n",
    "        stack.update(\n",
    "          (neighbor, current_depth + 1)\n",
    "          for direction in Direction.flat()\n",
    "          if (neighbor := self._get_neighbor(current, direction)) not in visited and not self._bugs_from_pos(neighbor) and bool(self._bugs_from_pos((right := self._get_neighbor(current, direction.right_of)))) != bool(self._bugs_from_pos((left := self._get_neighbor(current, direction.left_of)))) and right != origin != left\n",
    "        )\n",
    "    return {Move(bug, origin, destination) for destination in destinations if destination != origin}\n",
    "\n",
    "  def _get_beetle_moves(self, bug: Bug, origin: Position, virtual: bool = False) -> Set[Move]:\n",
    "    moves: Set[Move] = set()\n",
    "    for direction in Direction.flat():\n",
    "      # Don't consider the Beetle in the height, unless it's a virtual move (the bug is not actually in origin, but moving at the top of origin is part of its full move).\n",
    "      height = len(self._bugs_from_pos(origin)) - 1 + virtual\n",
    "      destination = self._get_neighbor(origin, direction)\n",
    "      dest_height = len(self._bugs_from_pos(destination))\n",
    "      left_height = len(self._bugs_from_pos(self._get_neighbor(origin, direction.left_of)))\n",
    "      right_height = len(self._bugs_from_pos(self._get_neighbor(origin, direction.right_of)))\n",
    "      # Logic from http://boardgamegeek.com/wiki/page/Hive_FAQ#toc9\n",
    "      if not ((height == 0 and dest_height == 0 and left_height == 0 and right_height == 0) or (dest_height < left_height and dest_height < right_height and height < left_height and height < right_height)):\n",
    "        moves.add(Move(bug, origin, destination))\n",
    "    return moves\n",
    "\n",
    "  def _get_grasshopper_moves(self, bug: Bug, origin: Position) -> Set[Move]:\n",
    "    moves: Set[Move] = set()\n",
    "    for direction in Direction.flat():\n",
    "      destination: Position = self._get_neighbor(origin, direction)\n",
    "      distance: int = 0\n",
    "      while self._bugs_from_pos(destination):\n",
    "        # Jump one more tile in the same direction\n",
    "        destination = self._get_neighbor(destination, direction)\n",
    "        distance += 1\n",
    "      if distance > 0:\n",
    "        # Can only move if there's at least one piece in the way\n",
    "        moves.add(Move(bug, origin, destination))\n",
    "    return moves\n",
    "\n",
    "  def _get_mosquito_moves(self, bug: Bug, origin: Position, special_only: bool = False) -> Set[Move]:\n",
    "    if len(self._bugs_from_pos(origin)) > 1:\n",
    "      return self._get_beetle_moves(bug, origin)\n",
    "    moves: Set[Move] = set()\n",
    "    bugs_copied: Set[BugType] = set()\n",
    "    for direction in Direction.flat():\n",
    "      if (bugs := self._bugs_from_pos(self._get_neighbor(origin, direction))) and (neighbor := bugs[-1]).type not in bugs_copied:\n",
    "         bugs_copied.add(neighbor.type)\n",
    "         if special_only:\n",
    "           if neighbor.type == BugType.PILLBUG:\n",
    "             moves.update(self._get_pillbug_special_moves(origin))\n",
    "         else:\n",
    "          match neighbor.type:\n",
    "            case BugType.QUEEN_BEE:\n",
    "              moves.update(self._get_sliding_moves(bug, origin, 1))\n",
    "            case BugType.SPIDER:\n",
    "              moves.update(self._get_sliding_moves(bug, origin, 3))\n",
    "            case BugType.BEETLE:\n",
    "              moves.update(self._get_beetle_moves(bug, origin))\n",
    "            case BugType.GRASSHOPPER:\n",
    "              moves.update(self._get_grasshopper_moves(bug, origin))\n",
    "            case BugType.SOLDIER_ANT:\n",
    "              moves.update(self._get_sliding_moves(bug, origin))\n",
    "            case BugType.LADYBUG:\n",
    "              moves.update(self._get_ladybug_moves(bug, origin))\n",
    "            case BugType.PILLBUG:\n",
    "              moves.update(self._get_sliding_moves(bug, origin, 1))\n",
    "            case BugType.MOSQUITO:\n",
    "              pass\n",
    "    return moves\n",
    "\n",
    "  def _get_ladybug_moves(self, bug: Bug, origin: Position) -> Set[Move]:\n",
    "    return {\n",
    "      Move(bug, origin, final_move.destination)\n",
    "      for first_move in self._get_beetle_moves(bug, origin, True) if self._bugs_from_pos(first_move.destination)\n",
    "      for second_move in self._get_beetle_moves(bug, first_move.destination, True) if self._bugs_from_pos(second_move.destination) and second_move.destination != origin\n",
    "      for final_move in self._get_beetle_moves(bug, second_move.destination, True) if not self._bugs_from_pos(final_move.destination) and final_move.destination != origin\n",
    "    }\n",
    "\n",
    "  def _get_pillbug_special_moves(self, origin: Position) -> Set[Move]:\n",
    "    moves: Set[Move] = set()\n",
    "    # There must be at least one empty neighboring tile for the Pillbug to move another bug piece\n",
    "    if (empty_positions := [self._get_neighbor(origin, direction) for direction in Direction.flat() if not self._bugs_from_pos(self._get_neighbor(origin, direction))]):\n",
    "      for direction in Direction.flat():\n",
    "        position = self._get_neighbor(origin, direction)\n",
    "        # A Pillbug can move another bug piece only if it's not stacked, it's not the last moved piece, it can be moved without breaking the hive, and it's not obstructed in moving above the Pillbug itself\n",
    "        if len(bugs := self._bugs_from_pos(position)) == 1 and self._was_not_last_moved(neighbor := bugs[-1]) and self._can_move_without_breaking_hive(position) and Move(neighbor, position, origin) in self._get_beetle_moves(neighbor, position):\n",
    "          moves.update(Move(neighbor, position, move.destination) for move in self._get_beetle_moves(neighbor, position, True) if move.destination in empty_positions)\n",
    "    return moves\n",
    "\n",
    "  def _can_move_without_breaking_hive(self, position: Position) -> bool:\n",
    "    # Try gaps heuristic first\n",
    "    neighbors: list[list[Bug]] = [self._bugs_from_pos(self._get_neighbor(position, direction)) for direction in Direction.flat()]\n",
    "    # If there is more than 1 gap, perform a DFS to check if all neighbors are still connected in some way.\n",
    "    if sum(bool(neighbors[i] and not neighbors[i - 1]) for i in range(len(neighbors))) > 1:\n",
    "      visited: Set[Position] = set()\n",
    "      neighbors_pos: list[Position] = [pos for bugs in neighbors if bugs and (pos := self._pos_from_bug(bugs[-1]))]    \n",
    "      stack: Set[Position] = {neighbors_pos[0]}\n",
    "      while stack:\n",
    "        current = stack.pop()\n",
    "        visited.add(current)\n",
    "        stack.update(neighbor for direction in Direction.flat() if (neighbor := self._get_neighbor(current, direction)) != position and self._bugs_from_pos(neighbor) and neighbor not in visited)\n",
    "      # Check if all neighbors with bug pieces were visited\n",
    "      return all(neighbor_pos in visited for neighbor_pos in neighbors_pos)\n",
    "    # If there is only 1 gap, then all neighboring pieces are connected even without the piece at the given position.\n",
    "    return True\n",
    "\n",
    "  def _can_bug_be_played(self, piece: Bug) -> bool:\n",
    "    return all(bug.id >= piece.id for bug, pos in self._bug_to_pos.items() if pos is None and bug.type is piece.type and bug.color is piece.color)\n",
    "\n",
    "  def _was_not_last_moved(self, bug: Bug) -> bool: \n",
    "    return not self.moves[-1] or self.moves[-1].bug != bug\n",
    "\n",
    "  def _is_bug_on_top(self, bug: Bug) -> bool:\n",
    "    return (pos := self._pos_from_bug(bug)) != None and self._bugs_from_pos(pos)[-1] == bug\n",
    "\n",
    "  def _bugs_from_pos(self, position: Position) -> list[Bug]:\n",
    "    return self._pos_to_bug[position] if position in self._pos_to_bug else []\n",
    "\n",
    "  def _pos_from_bug(self, bug: Bug) -> Optional[Position]:\n",
    "    return self._bug_to_pos[bug] if bug in self._bug_to_pos else None\n",
    "\n",
    "  def _get_neighbor(self, position: Position, direction: Direction) -> Position:\n",
    "    return position + self.NEIGHBOR_DELTAS[direction.delta_index]\n",
    "  \n",
    "  def get_opponent(self, player):\n",
    "    return -player\n",
    "  \n",
    "  def get_opponent_value(self, value):\n",
    "    return -value\n",
    "  \n",
    "  def change_perspective(self, state, player):\n",
    "    return state * player\n",
    "  \n",
    "  def get_encoded_state(self, state):\n",
    "    encoded_state = np.stack(\n",
    "        (state if state < 0 else 0, state == 0, state if state > 0 else 0)\n",
    "    ).astype(np.float32)\n",
    "    \n",
    "    if len(state.shape) == 3:\n",
    "      encoded_state = np.swapaxes(encoded_state, 0, 1)\n",
    "    \n",
    "    return encoded_state\n",
    "\n",
    "  def state(self):\n",
    "      return self.board\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2cb2023a-a481-423b-9138-b2bfcafd9b0d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m game \u001b[38;5;241m=\u001b[39m \u001b[43mHiveBoard\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBase+MLP\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m player \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn[5], line 23\u001b[0m, in \u001b[0;36mHiveBoard.__init__\u001b[1;34m(self, gamestring)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheight_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_moved : Bug \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mboard \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrow_count, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumn_count, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheight_count), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bug_to_pos: \u001b[38;5;28mdict\u001b[39m[Bug, Optional[Position]] \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m color \u001b[38;5;129;01min\u001b[39;00m PlayerColor:\n\u001b[0;32m     27\u001b[0m   \u001b[38;5;66;03m#for expansion in self.type:\u001b[39;00m\n\u001b[0;32m     28\u001b[0m    \u001b[38;5;66;03m# if expansion is GameType.Base:\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "game = HiveBoard(\"Base+MLP\")\n",
    "player = 1\n",
    "\n",
    "\n",
    "while True:\n",
    "    print(game.state())\n",
    "    \n",
    "    valid_moves = game.get_valid_moves()\n",
    "    print(valid_moves)\n",
    "    print(\"valid_moves\", [valid_moves[i] for i in range(len(valid_moves))])\n",
    "    action = int(input(f\"{player}:\"))\n",
    "\n",
    "    if valid_moves[action] == 0:\n",
    "        print(\"action not valid\")\n",
    "        continue\n",
    "            \n",
    "    state = game.get_next_state(state, action, player)\n",
    "    \n",
    "    value, is_terminal = game.get_value_and_terminated(state, action)\n",
    "    \n",
    "    if is_terminal:\n",
    "        print(state)\n",
    "        if value == 1:\n",
    "            print(player, \"won\")\n",
    "        else:\n",
    "            print(\"draw\")\n",
    "        break\n",
    "        \n",
    "    player = game.get_opponent(player)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1761699",
   "metadata": {},
   "source": [
    "### Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02e5b58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, game, num_resBlocks, num_hidden, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        self.startBlock = nn.Sequential(\n",
    "            nn.Conv2d(3, num_hidden, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(num_hidden),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.backBone = nn.ModuleList(\n",
    "            [ResBlock(num_hidden) for i in range(num_resBlocks)]\n",
    "        )\n",
    "        \n",
    "        self.policyHead = nn.Sequential(\n",
    "            nn.Conv2d(num_hidden, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32 * game.row_count * game.column_count, game.action_size)\n",
    "        )\n",
    "        \n",
    "        self.valueHead = nn.Sequential(\n",
    "            nn.Conv2d(num_hidden, 3, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(3),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3 * game.row_count * game.column_count, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        self.to(device)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.startBlock(x)\n",
    "        for resBlock in self.backBone:\n",
    "            x = resBlock(x)\n",
    "        policy = self.policyHead(x)\n",
    "        value = self.valueHead(x)\n",
    "        return policy, value\n",
    "        \n",
    "        \n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, num_hidden):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(num_hidden)\n",
    "        self.conv2 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(num_hidden)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.bn2(self.conv2(x))\n",
    "        x += residual\n",
    "        x = F.relu(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25817615",
   "metadata": {},
   "source": [
    "### Basic MCTS and Alpha Zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21866526",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, game, args, state, parent=None, action_taken=None, prior=0, visit_count=0):\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.state = state\n",
    "        self.parent = parent\n",
    "        self.action_taken = action_taken\n",
    "        self.prior = prior\n",
    "        \n",
    "        self.children = []\n",
    "        \n",
    "        self.visit_count = visit_count\n",
    "        self.value_sum = 0\n",
    "        \n",
    "    def is_fully_expanded(self):\n",
    "        return len(self.children) > 0\n",
    "    \n",
    "    def select(self):\n",
    "        best_child = None\n",
    "        best_ucb = -np.inf\n",
    "        \n",
    "        for child in self.children:\n",
    "            ucb = self.get_ucb(child)\n",
    "            if ucb > best_ucb:\n",
    "                best_child = child\n",
    "                best_ucb = ucb\n",
    "                \n",
    "        return best_child\n",
    "    \n",
    "    def get_ucb(self, child):\n",
    "        if child.visit_count == 0:\n",
    "            q_value = 0\n",
    "        else:\n",
    "            q_value = 1 - ((child.value_sum / child.visit_count) + 1) / 2\n",
    "        return q_value + self.args['C'] * (math.sqrt(self.visit_count) / (child.visit_count + 1)) * child.prior\n",
    "    \n",
    "    def expand(self, policy):\n",
    "        for action, prob in enumerate(policy):\n",
    "            if prob > 0:\n",
    "                child_state = self.state.copy()\n",
    "                child_state = self.game.get_next_state(child_state, action, 1)\n",
    "                child_state = self.game.change_perspective(child_state, player=-1)\n",
    "\n",
    "                child = Node(self.game, self.args, child_state, self, action, prob)\n",
    "                self.children.append(child)\n",
    "                \n",
    "        return child\n",
    "            \n",
    "    def backpropagate(self, value):\n",
    "        self.value_sum += value\n",
    "        self.visit_count += 1\n",
    "        \n",
    "        value = self.game.get_opponent_value(value)\n",
    "        if self.parent is not None:\n",
    "            self.parent.backpropagate(value)  \n",
    "\n",
    "\n",
    "class MCTS:\n",
    "    def __init__(self, game, args, model):\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.model = model\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def search(self, state):\n",
    "        root = Node(self.game, self.args, state, visit_count=1)\n",
    "        \n",
    "        policy, _ = self.model(\n",
    "            torch.tensor(self.game.get_encoded_state(state), device=self.model.device).unsqueeze(0)\n",
    "        )\n",
    "        policy = torch.softmax(policy, axis=1).squeeze(0).cpu().numpy()\n",
    "        policy = (1 - self.args['dirichlet_epsilon']) * policy + self.args['dirichlet_epsilon'] \\\n",
    "            * np.random.dirichlet([self.args['dirichlet_alpha']] * self.game.action_size)\n",
    "        \n",
    "        valid_moves = self.game.get_valid_moves(state)\n",
    "        policy *= valid_moves\n",
    "        policy /= np.sum(policy)\n",
    "        root.expand(policy)\n",
    "        \n",
    "        for search in range(self.args['num_searches']):\n",
    "            node = root\n",
    "            \n",
    "            while node.is_fully_expanded():\n",
    "                node = node.select()\n",
    "                \n",
    "            value, is_terminal = self.game.get_value_and_terminated(node.state, node.action_taken)\n",
    "            value = self.game.get_opponent_value(value)\n",
    "            \n",
    "            if not is_terminal:\n",
    "                policy, value = self.model(\n",
    "                    torch.tensor(self.game.get_encoded_state(node.state), device=self.model.device).unsqueeze(0)\n",
    "                )\n",
    "                policy = torch.softmax(policy, axis=1).squeeze(0).cpu().numpy()\n",
    "                valid_moves = self.game.get_valid_moves(node.state)\n",
    "                policy *= valid_moves\n",
    "                policy /= np.sum(policy)\n",
    "                \n",
    "                value = value.item()\n",
    "                \n",
    "                node.expand(policy)\n",
    "                \n",
    "            node.backpropagate(value)    \n",
    "            \n",
    "            \n",
    "        action_probs = np.zeros(self.game.action_size)\n",
    "        for child in root.children:\n",
    "            action_probs[child.action_taken] = child.visit_count\n",
    "        action_probs /= np.sum(action_probs)\n",
    "        return action_probs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3b28ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaZero:\n",
    "    def __init__(self, model, optimizer, game, args):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.mcts = MCTS(game, args, model)\n",
    "        \n",
    "    def selfPlay(self):\n",
    "        memory = []\n",
    "        player = 1\n",
    "        state = self.game.get_initial_state()\n",
    "        \n",
    "        while True:\n",
    "            neutral_state = self.game.change_perspective(state, player)\n",
    "            action_probs = self.mcts.search(neutral_state)\n",
    "            \n",
    "            memory.append((neutral_state, action_probs, player))\n",
    "            \n",
    "            temperature_action_probs = action_probs ** (1 / self.args['temperature'])\n",
    "            action = np.random.choice(self.game.action_size, p=temperature_action_probs) # Divide temperature_action_probs with its sum in case of an error\n",
    "            \n",
    "            state = self.game.get_next_state(state, action, player)\n",
    "            \n",
    "            value, is_terminal = self.game.get_value_and_terminated(state, action)\n",
    "            \n",
    "            if is_terminal:\n",
    "                returnMemory = []\n",
    "                for hist_neutral_state, hist_action_probs, hist_player in memory:\n",
    "                    hist_outcome = value if hist_player == player else self.game.get_opponent_value(value)\n",
    "                    returnMemory.append((\n",
    "                        self.game.get_encoded_state(hist_neutral_state),\n",
    "                        hist_action_probs,\n",
    "                        hist_outcome\n",
    "                    ))\n",
    "                return returnMemory\n",
    "            \n",
    "            player = self.game.get_opponent(player)\n",
    "                \n",
    "    def train(self, memory):\n",
    "        random.shuffle(memory)\n",
    "        for batchIdx in range(0, len(memory), self.args['batch_size']):\n",
    "            sample = memory[batchIdx:min(len(memory) - 1, batchIdx + self.args['batch_size'])] # Change to memory[batchIdx:batchIdx+self.args['batch_size']] in case of an error\n",
    "            state, policy_targets, value_targets = zip(*sample)\n",
    "            \n",
    "            state, policy_targets, value_targets = np.array(state), np.array(policy_targets), np.array(value_targets).reshape(-1, 1)\n",
    "            \n",
    "            state = torch.tensor(state, dtype=torch.float32, device=self.model.device)\n",
    "            policy_targets = torch.tensor(policy_targets, dtype=torch.float32, device=self.model.device)\n",
    "            value_targets = torch.tensor(value_targets, dtype=torch.float32, device=self.model.device)\n",
    "            \n",
    "            out_policy, out_value = self.model(state)\n",
    "            \n",
    "            policy_loss = F.cross_entropy(out_policy, policy_targets)\n",
    "            value_loss = F.mse_loss(out_value, value_targets)\n",
    "            loss = policy_loss + value_loss\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "    \n",
    "    def learn(self):\n",
    "        for iteration in range(self.args['num_iterations']):\n",
    "            memory = []\n",
    "            \n",
    "            self.model.eval()\n",
    "            for selfPlay_iteration in trange(self.args['num_selfPlay_iterations']):\n",
    "                memory += self.selfPlay()\n",
    "                \n",
    "            self.model.train()\n",
    "            for epoch in trange(self.args['num_epochs']):\n",
    "                self.train(memory)\n",
    "            \n",
    "            torch.save(self.model.state_dict(), f\"model_{iteration}_{self.game}.pt\")\n",
    "            torch.save(self.optimizer.state_dict(), f\"optimizer_{iteration}_{self.game}.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3248e3",
   "metadata": {},
   "source": [
    "### Optimized and parallel (even if without threads) version of MCTS and Alpha Zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e997f3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCTSParallel:\n",
    "    def __init__(self, game, args, model):\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.model = model\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def search(self, states, spGames):\n",
    "        policy, _ = self.model(\n",
    "            torch.tensor(self.game.get_encoded_state(states), device=self.model.device)\n",
    "        )\n",
    "        policy = torch.softmax(policy, axis=1).cpu().numpy()\n",
    "        policy = (1 - self.args['dirichlet_epsilon']) * policy + self.args['dirichlet_epsilon'] \\\n",
    "            * np.random.dirichlet([self.args['dirichlet_alpha']] * self.game.action_size, size=policy.shape[0])\n",
    "        \n",
    "        for i, spg in enumerate(spGames):\n",
    "            spg_policy = policy[i]\n",
    "            valid_moves = self.game.get_valid_moves(states[i])\n",
    "            spg_policy *= valid_moves\n",
    "            spg_policy /= np.sum(spg_policy)\n",
    "\n",
    "            spg.root = Node(self.game, self.args, states[i], visit_count=1)\n",
    "            spg.root.expand(spg_policy)\n",
    "        \n",
    "        for search in range(self.args['num_searches']):\n",
    "            for spg in spGames:\n",
    "                spg.node = None\n",
    "                node = spg.root\n",
    "\n",
    "                while node.is_fully_expanded():\n",
    "                    node = node.select()\n",
    "\n",
    "                value, is_terminal = self.game.get_value_and_terminated(node.state, node.action_taken)\n",
    "                value = self.game.get_opponent_value(value)\n",
    "                \n",
    "                if is_terminal:\n",
    "                    node.backpropagate(value)\n",
    "                    \n",
    "                else:\n",
    "                    spg.node = node\n",
    "                    \n",
    "            expandable_spGames = [mappingIdx for mappingIdx in range(len(spGames)) if spGames[mappingIdx].node is not None]\n",
    "                    \n",
    "            if len(expandable_spGames) > 0:\n",
    "                states = np.stack([spGames[mappingIdx].node.state for mappingIdx in expandable_spGames])\n",
    "                \n",
    "                policy, value = self.model(\n",
    "                    torch.tensor(self.game.get_encoded_state(states), device=self.model.device)\n",
    "                )\n",
    "                policy = torch.softmax(policy, axis=1).cpu().numpy()\n",
    "                value = value.cpu().numpy()\n",
    "                \n",
    "            for i, mappingIdx in enumerate(expandable_spGames):\n",
    "                node = spGames[mappingIdx].node\n",
    "                spg_policy, spg_value = policy[i], value[i]\n",
    "                \n",
    "                valid_moves = self.game.get_valid_moves(node.state)\n",
    "                spg_policy *= valid_moves\n",
    "                spg_policy /= np.sum(spg_policy)\n",
    "\n",
    "                node.expand(spg_policy)\n",
    "                node.backpropagate(spg_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d0a5a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaZeroParallel:\n",
    "    def __init__(self, model, optimizer, game, args):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.mcts = MCTSParallel(game, args, model)\n",
    "        \n",
    "    def selfPlay(self):\n",
    "        return_memory = []\n",
    "        player = 1\n",
    "        spGames = [SPG(self.game) for spg in range(self.args['num_parallel_games'])]\n",
    "        \n",
    "        while len(spGames) > 0:\n",
    "            states = np.stack([spg.state for spg in spGames])\n",
    "            neutral_states = self.game.change_perspective(states, player)\n",
    "            \n",
    "            self.mcts.search(neutral_states, spGames)\n",
    "            \n",
    "            for i in range(len(spGames))[::-1]:\n",
    "                spg = spGames[i]\n",
    "                \n",
    "                action_probs = np.zeros(self.game.action_size)\n",
    "                for child in spg.root.children:\n",
    "                    action_probs[child.action_taken] = child.visit_count\n",
    "                action_probs /= np.sum(action_probs)\n",
    "\n",
    "                spg.memory.append((spg.root.state, action_probs, player))\n",
    "\n",
    "                temperature_action_probs = action_probs ** (1 / self.args['temperature'])\n",
    "                action = np.random.choice(self.game.action_size, p=temperature_action_probs) # Divide temperature_action_probs with its sum in case of an error\n",
    "\n",
    "                spg.state = self.game.get_next_state(spg.state, action, player)\n",
    "\n",
    "                value, is_terminal = self.game.get_value_and_terminated(spg.state, action)\n",
    "\n",
    "                if is_terminal:\n",
    "                    for hist_neutral_state, hist_action_probs, hist_player in spg.memory:\n",
    "                        hist_outcome = value if hist_player == player else self.game.get_opponent_value(value)\n",
    "                        return_memory.append((\n",
    "                            self.game.get_encoded_state(hist_neutral_state),\n",
    "                            hist_action_probs,\n",
    "                            hist_outcome\n",
    "                        ))\n",
    "                    del spGames[i]\n",
    "                    \n",
    "            player = self.game.get_opponent(player)\n",
    "            \n",
    "        return return_memory\n",
    "                \n",
    "    def train(self, memory):\n",
    "        random.shuffle(memory)\n",
    "        for batchIdx in range(0, len(memory), self.args['batch_size']):\n",
    "            sample = memory[batchIdx:min(len(memory) - 1, batchIdx + self.args['batch_size'])] # Change to memory[batchIdx:batchIdx+self.args['batch_size']] in case of an error\n",
    "            state, policy_targets, value_targets = zip(*sample)\n",
    "            \n",
    "            state, policy_targets, value_targets = np.array(state), np.array(policy_targets), np.array(value_targets).reshape(-1, 1)\n",
    "            \n",
    "            state = torch.tensor(state, dtype=torch.float32, device=self.model.device)\n",
    "            policy_targets = torch.tensor(policy_targets, dtype=torch.float32, device=self.model.device)\n",
    "            value_targets = torch.tensor(value_targets, dtype=torch.float32, device=self.model.device)\n",
    "            \n",
    "            out_policy, out_value = self.model(state)\n",
    "            \n",
    "            policy_loss = F.cross_entropy(out_policy, policy_targets)\n",
    "            value_loss = F.mse_loss(out_value, value_targets)\n",
    "            loss = policy_loss + value_loss\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "    \n",
    "    def learn(self):\n",
    "        for iteration in range(self.args['num_iterations']):\n",
    "            memory = []\n",
    "            \n",
    "            self.model.eval()\n",
    "            for selfPlay_iteration in trange(self.args['num_selfPlay_iterations'] // self.args['num_parallel_games']):\n",
    "                memory += self.selfPlay()\n",
    "                \n",
    "            self.model.train()\n",
    "            for epoch in trange(self.args['num_epochs']):\n",
    "                self.train(memory)\n",
    "            \n",
    "            torch.save(self.model.state_dict(), f\"model_{iteration}_{self.game}.pt\")\n",
    "            torch.save(self.optimizer.state_dict(), f\"optimizer_{iteration}_{self.game}.pt\")\n",
    "            \n",
    "class SPG:\n",
    "    def __init__(self, game):\n",
    "        self.state = game.get_initial_state()\n",
    "        self.memory = []\n",
    "        self.root = None\n",
    "        self.node = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26531e0",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bd91ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "game = ConnectFour()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = ResNet(game, 9, 128, device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "\n",
    "args = {\n",
    "    'C': 2,\n",
    "    'num_searches': 600,\n",
    "    'num_iterations': 8,\n",
    "    'num_selfPlay_iterations': 500,\n",
    "    'num_parallel_games': 100,\n",
    "    'num_epochs': 4,\n",
    "    'batch_size': 128,\n",
    "    'temperature': 1.25,\n",
    "    'dirichlet_epsilon': 0.25,\n",
    "    'dirichlet_alpha': 0.3\n",
    "}\n",
    "\n",
    "alphaZero = AlphaZeroParallel(model, optimizer, game, args)\n",
    "alphaZero.learn()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da25d96",
   "metadata": {},
   "source": [
    "### Test to see how it plays\n",
    "\n",
    "To make it play we only need the model (ResNet), the file produced by the training (.pt file) and the MCTS algorithm (standard version)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c470145",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "game = ConnectFour()\n",
    "player = 1\n",
    "\n",
    "args = {\n",
    "    'C': 2,\n",
    "    'num_searches': 600,\n",
    "    'dirichlet_epsilon': 0.,\n",
    "    'dirichlet_alpha': 0.3\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = ResNet(game, 9, 128, device)\n",
    "model.load_state_dict(torch.load(\"model_7_ConnectFour.pt\", map_location=device))\n",
    "model.eval()\n",
    "\n",
    "mcts = MCTS(game, args, model)\n",
    "\n",
    "state = game.get_initial_state()\n",
    "\n",
    "\n",
    "while True:\n",
    "    print(state)\n",
    "    \n",
    "    if player == 1:\n",
    "        valid_moves = game.get_valid_moves(state)\n",
    "        print(\"valid_moves\", [i for i in range(game.action_size) if valid_moves[i] == 1])\n",
    "        action = int(input(f\"{player}:\"))\n",
    "\n",
    "        if valid_moves[action] == 0:\n",
    "            print(\"action not valid\")\n",
    "            continue\n",
    "            \n",
    "    else:\n",
    "        neutral_state = game.change_perspective(state, player)\n",
    "        mcts_probs = mcts.search(neutral_state)\n",
    "        action = np.argmax(mcts_probs)\n",
    "        \n",
    "    state = game.get_next_state(state, action, player)\n",
    "    \n",
    "    value, is_terminal = game.get_value_and_terminated(state, action)\n",
    "    \n",
    "    if is_terminal:\n",
    "        print(state)\n",
    "        if value == 1:\n",
    "            print(player, \"won\")\n",
    "        else:\n",
    "            print(\"draw\")\n",
    "        break\n",
    "        \n",
    "    player = game.get_opponent(player)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "2177f1ca12c1330a133c1d40b46100b268ab447cddcbdfdc0c7b2b7e4840e700"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
