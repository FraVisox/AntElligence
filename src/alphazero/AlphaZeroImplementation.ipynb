{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6a2b6a16",
      "metadata": {
        "id": "6a2b6a16"
      },
      "source": [
        "# Implementation of Alpha Zero for games TicTacToe and Connect 4\n",
        "This follows the ideas of idea.md"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d090f5e",
      "metadata": {
        "id": "2d090f5e",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from tqdm.notebook import trange\n",
        "\n",
        "import random\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57524aa1",
      "metadata": {
        "id": "57524aa1"
      },
      "source": [
        "### Games"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a097e1b5",
      "metadata": {
        "id": "a097e1b5"
      },
      "outputs": [],
      "source": [
        "class TicTacToe:\n",
        "    def __init__(self):\n",
        "        self.row_count = 3\n",
        "        self.column_count = 3\n",
        "        self.action_size = self.row_count * self.column_count\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"TicTacToe\"\n",
        "\n",
        "    def get_initial_state(self):\n",
        "        return np.zeros((self.row_count, self.column_count))\n",
        "\n",
        "    def get_next_state(self, state, action, player):\n",
        "        row = action // self.column_count\n",
        "        column = action % self.column_count\n",
        "        state[row, column] = player\n",
        "        return state\n",
        "\n",
        "    def get_valid_moves(self, state):\n",
        "        return (state.reshape(-1) == 0).astype(np.uint8)\n",
        "\n",
        "    def check_win(self, state, action):\n",
        "        if action == None:\n",
        "            return False\n",
        "\n",
        "        row = action // self.column_count\n",
        "        column = action % self.column_count\n",
        "        player = state[row, column]\n",
        "\n",
        "        return (\n",
        "            np.sum(state[row, :]) == player * self.column_count\n",
        "            or np.sum(state[:, column]) == player * self.row_count\n",
        "            or np.sum(np.diag(state)) == player * self.row_count\n",
        "            or np.sum(np.diag(np.flip(state, axis=0))) == player * self.row_count\n",
        "        )\n",
        "\n",
        "    def get_value_and_terminated(self, state, action):\n",
        "        if self.check_win(state, action):\n",
        "            return 1, True\n",
        "        if np.sum(self.get_valid_moves(state)) == 0:\n",
        "            return 0, True\n",
        "        return 0, False\n",
        "\n",
        "    def get_opponent(self, player):\n",
        "        return -player\n",
        "\n",
        "    def get_opponent_value(self, value):\n",
        "        return -value\n",
        "\n",
        "    def change_perspective(self, state, player):\n",
        "        return state * player\n",
        "\n",
        "    def get_encoded_state(self, state):\n",
        "        encoded_state = np.stack(\n",
        "            (state == -1, state == 0, state == 1)\n",
        "        ).astype(np.float32)\n",
        "\n",
        "        if len(state.shape) == 3:\n",
        "            encoded_state = np.swapaxes(encoded_state, 0, 1)\n",
        "\n",
        "        return encoded_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "682c4ca4",
      "metadata": {
        "id": "682c4ca4"
      },
      "outputs": [],
      "source": [
        "class ConnectFour:\n",
        "    def __init__(self):\n",
        "        self.row_count = 6\n",
        "        self.column_count = 7\n",
        "        self.action_size = self.column_count\n",
        "        self.in_a_row = 4\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"ConnectFour\"\n",
        "\n",
        "    def get_initial_state(self):\n",
        "        return np.zeros((self.row_count, self.column_count))\n",
        "\n",
        "    def get_next_state(self, state, action, player):\n",
        "        row = np.max(np.where(state[:, action] == 0))\n",
        "        state[row, action] = player\n",
        "        return state\n",
        "\n",
        "    def get_valid_moves(self, state):\n",
        "        return (state[0] == 0).astype(np.uint8)\n",
        "\n",
        "    def check_win(self, state, action):\n",
        "        if action == None:\n",
        "            return False\n",
        "\n",
        "        row = np.min(np.where(state[:, action] != 0))\n",
        "        column = action\n",
        "        player = state[row][column]\n",
        "\n",
        "        def count(offset_row, offset_column):\n",
        "            for i in range(1, self.in_a_row):\n",
        "                r = row + offset_row * i\n",
        "                c = action + offset_column * i\n",
        "                if (\n",
        "                    r < 0\n",
        "                    or r >= self.row_count\n",
        "                    or c < 0\n",
        "                    or c >= self.column_count\n",
        "                    or state[r][c] != player\n",
        "                ):\n",
        "                    return i - 1\n",
        "            return self.in_a_row - 1\n",
        "\n",
        "        return (\n",
        "            count(1, 0) >= self.in_a_row - 1 # vertical\n",
        "            or (count(0, 1) + count(0, -1)) >= self.in_a_row - 1 # horizontal\n",
        "            or (count(1, 1) + count(-1, -1)) >= self.in_a_row - 1 # top left diagonal\n",
        "            or (count(1, -1) + count(-1, 1)) >= self.in_a_row - 1 # top right diagonal\n",
        "        )\n",
        "\n",
        "    def get_value_and_terminated(self, state, action):\n",
        "        if self.check_win(state, action):\n",
        "            return 1, True\n",
        "        if np.sum(self.get_valid_moves(state)) == 0:\n",
        "            return 0, True\n",
        "        return 0, False\n",
        "\n",
        "    def get_opponent(self, player):\n",
        "        return -player\n",
        "\n",
        "    def get_opponent_value(self, value):\n",
        "        return -value\n",
        "\n",
        "    def change_perspective(self, state, player):\n",
        "        return state * player\n",
        "\n",
        "    def get_encoded_state(self, state):\n",
        "        encoded_state = np.stack(\n",
        "            (state == -1, state == 0, state == 1)\n",
        "        ).astype(np.float32)\n",
        "\n",
        "        if len(state.shape) == 3:\n",
        "            encoded_state = np.swapaxes(encoded_state, 0, 1)\n",
        "\n",
        "        return encoded_state"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc1043ac",
      "metadata": {
        "id": "fc1043ac"
      },
      "source": [
        "### OUR GAME"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gU_nu7Ecwutr",
      "metadata": {
        "id": "gU_nu7Ecwutr"
      },
      "outputs": [],
      "source": [
        "# compile with  g++ -shared -o engine.dll *.cpp -static -static-libgcc -static-libstdc++ -I. -O2 -Wall -DBUILDING_DLL\n",
        "# for linux compile with g++ -fPIC -shared -o engine.dll *.cpp\n",
        "\n",
        "import ctypes\n",
        "import os\n",
        "\n",
        "# This represents the return types of the functions\n",
        "class ReturnTypes:\n",
        "    OK = 0\n",
        "    ERROR = 1\n",
        "    GAME_OVER_DRAW = 2\n",
        "    GAME_OVER_WHITE_WINS = 3\n",
        "    GAME_OVER_BLACK_WINS = 4\n",
        "    INVALID_ARGUMENT = 5\n",
        "    INVALID_GAME_NOT_STARTED = 6\n",
        "\n",
        "class EngineDLL():\n",
        "    def __init__(self,gamestring: str = \"\"):\n",
        "        #TODO: understand how to make this shared dll available in colab\n",
        "        path = os.path.join(os.getcwd(), \"engine_cpp/engine.dll\")\n",
        "        self.dll = ctypes.CDLL(path)\n",
        "        self._setup_functions()\n",
        "        self.start_game(gamestring)\n",
        "\n",
        "    def _setup_functions(self):\n",
        "        # Set up game functions\n",
        "        self.dll.startGame.argtypes = [ctypes.c_char_p]\n",
        "        self.dll.startGame.restype = ctypes.c_int\n",
        "\n",
        "        self.dll.playMove.argtypes = [ctypes.c_char_p]\n",
        "        self.dll.playMove.restype = ctypes.c_int\n",
        "\n",
        "        self.dll.validMoves.restype = ctypes.c_char_p\n",
        "\n",
        "        self.dll.getBoard.restype = ctypes.c_char_p\n",
        "\n",
        "        self.dll.undo.argtypes = [ctypes.c_int]\n",
        "\n",
        "        self.dll.getTurn.restype = ctypes.c_int\n",
        "\n",
        "        self.dll.oracleEval.restype=ctypes.c_float\n",
        "\n",
        "    def current_player_turn(self)->int:\n",
        "        return self.dll.getTurn()\n",
        "\n",
        "    def valid_moves(self)->str:\n",
        "        return self.get_valid_moves()\n",
        "\n",
        "    def play(self,move_string: str)-> None:\n",
        "        self.play_move(move_string)\n",
        "\n",
        "    def start_game(self, game_string):\n",
        "        encoded_string = game_string.encode(\"utf-8\")\n",
        "        return self.dll.startGame(encoded_string) == ReturnTypes.OK\n",
        "\n",
        "    def play_move(self, move_string):\n",
        "        encoded_string = move_string.encode(\"utf-8\")\n",
        "        return self.dll.playMove(encoded_string)\n",
        "\n",
        "    def get_valid_moves(self):\n",
        "        try:\n",
        "            result = self.dll.validMoves()\n",
        "            if result:\n",
        "                raw_bytes = ctypes.string_at(result)\n",
        "                try:\n",
        "                    decoded = raw_bytes.decode(\"utf-8\")\n",
        "                    return decoded\n",
        "                except UnicodeDecodeError as e:\n",
        "                    return raw_bytes.decode('ascii', errors='replace')\n",
        "        except Exception:\n",
        "            return \"\"\n",
        "\n",
        "    def get_board(self):\n",
        "        result = self.dll.getBoard()\n",
        "        if result:\n",
        "            try:\n",
        "                return ctypes.string_at(result).decode(\"utf-8\")\n",
        "            except UnicodeDecodeError:\n",
        "                return ctypes.string_at(result).decode('ascii', errors='replace')\n",
        "        return \"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Z4oPS1j1uv70",
      "metadata": {
        "id": "Z4oPS1j1uv70"
      },
      "outputs": [],
      "source": [
        "## ENGINE\n",
        "class Engine():\n",
        "\n",
        "  def __init__(self):\n",
        "    # Create a singleton instance\n",
        "    self.CPPInterface = EngineDLL()\n",
        "\n",
        "  # Init the game\n",
        "  def newgame(self, arguments: list[str]) -> None:\n",
        "    self.CPPInterface.start_game(\" \".join(arguments))\n",
        "\n",
        "  ##TODO: define a state and a board in a convenient way\n",
        "\n",
        "  def get_initial_state(self):\n",
        "    self.newgame(\"Base+MLP\")\n",
        "    return self.CPPInterface.get_board()\n",
        "\n",
        "  #TODO: how are these passed?\n",
        "  def get_valid_moves(self, state):\n",
        "    return self.CPPInterface.get_valid_moves(state) #TODO: does it make sense to pass the state here?\n",
        "\n",
        "  def get_next_state(self, state, action, player):\n",
        "    self.CPPInterface.play_move(state, action) #Player is useless\n",
        "    return self.CPPInterface.get_board()\n",
        "\n",
        "  def check_win(self, state): #TODO: check if this action gets to a winning state\n",
        "    return self.CPPInterface.check_win(state)\n",
        "\n",
        "  def get_value_and_terminated(self, state): #TODO: fix, maybe it's finished and we just lost\n",
        "        if self.check_win(state):\n",
        "            return 1, True\n",
        "        if np.sum(self.get_valid_moves(state)) == 0:\n",
        "            return 0, True\n",
        "        return 0, False\n",
        "\n",
        "  def get_opponent(self, player):\n",
        "        return -player\n",
        "\n",
        "  def get_opponent_value(self, value):\n",
        "      return -value\n",
        "\n",
        "  def change_perspective(self, state, player):\n",
        "      return state * player\n",
        "\n",
        "  def get_encoded_state(self, state):\n",
        "      encoded_state = np.stack(\n",
        "          (state == -1, state == 0, state == 1)\n",
        "      ).astype(np.float32)\n",
        "\n",
        "      if len(state.shape) == 3:\n",
        "          encoded_state = np.swapaxes(encoded_state, 0, 1)\n",
        "\n",
        "      return encoded_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2cb2023a-a481-423b-9138-b2bfcafd9b0d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "2cb2023a-a481-423b-9138-b2bfcafd9b0d",
        "outputId": "88663c65-9a0f-4598-c2a8-58bba4255aa9"
      },
      "outputs": [
        {
          "ename": "OSError",
          "evalue": "/content/engine_cpp/engine.dll: cannot open shared object file: No such file or directory",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-4312c8636424>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEngine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_initial_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-b3095a76a157>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Create a singleton instance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCPPInterface\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEngineDLL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0;31m# Init the game\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-864272c708c6>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, gamestring)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m#TODO: understand how to make this shared dll available in colab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"engine_cpp/engine.dll\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCDLL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_functions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_game\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgamestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/ctypes/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_dlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: /content/engine_cpp/engine.dll: cannot open shared object file: No such file or directory"
          ]
        }
      ],
      "source": [
        "game = Engine()\n",
        "player = 1\n",
        "\n",
        "state = game.get_initial_state()\n",
        "print(state)\n",
        "\n",
        "while True:\n",
        "\n",
        "    print(\"Let's get the valid moves\")\n",
        "    valid_moves = list(game.get_valid_moves(state))\n",
        "    print(\"Valid moves are \", len(valid_moves), \" and they are\\n\", valid_moves)\n",
        "    index = int(input(f\"Enter the index of the move for {player}:\"))\n",
        "\n",
        "    action = valid_moves[index]\n",
        "\n",
        "    state = game.get_next_state(state, action, player)\n",
        "\n",
        "    value, is_terminal = game.get_value_and_terminated(state)\n",
        "\n",
        "    if is_terminal:\n",
        "        print(game.state())\n",
        "        if value == 1:\n",
        "            print(player, \"won\")\n",
        "        else:\n",
        "            print(\"draw\")\n",
        "        break\n",
        "\n",
        "    player = game.get_opponent(player)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1761699",
      "metadata": {
        "id": "a1761699"
      },
      "source": [
        "### Neural Network Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02e5b58b",
      "metadata": {
        "id": "02e5b58b"
      },
      "outputs": [],
      "source": [
        "class ResNet(nn.Module):\n",
        "    def __init__(self, game, num_resBlocks, num_hidden, device):\n",
        "        super().__init__()\n",
        "\n",
        "        self.device = device\n",
        "        self.startBlock = nn.Sequential(\n",
        "            nn.Conv2d(3, num_hidden, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(num_hidden),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.backBone = nn.ModuleList(\n",
        "            [ResBlock(num_hidden) for i in range(num_resBlocks)]\n",
        "        )\n",
        "\n",
        "        self.policyHead = nn.Sequential(\n",
        "            nn.Conv2d(num_hidden, 32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(32 * game.row_count * game.column_count, game.action_size)\n",
        "        )\n",
        "\n",
        "        self.valueHead = nn.Sequential(\n",
        "            nn.Conv2d(num_hidden, 3, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(3),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(3 * game.row_count * game.column_count, 1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "        self.to(device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.startBlock(x)\n",
        "        for resBlock in self.backBone:\n",
        "            x = resBlock(x)\n",
        "        policy = self.policyHead(x)\n",
        "        value = self.valueHead(x)\n",
        "        return policy, value\n",
        "\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, num_hidden):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(num_hidden)\n",
        "        self.conv2 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(num_hidden)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = self.bn2(self.conv2(x))\n",
        "        x += residual\n",
        "        x = F.relu(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25817615",
      "metadata": {
        "id": "25817615"
      },
      "source": [
        "### Basic MCTS and Alpha Zero"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21866526",
      "metadata": {
        "id": "21866526"
      },
      "outputs": [],
      "source": [
        "class Node:\n",
        "    def __init__(self, game, args, state, parent=None, action_taken=None, prior=0, visit_count=0):\n",
        "        self.game = game\n",
        "        self.args = args\n",
        "        self.state = state\n",
        "        self.parent = parent\n",
        "        self.action_taken = action_taken\n",
        "        self.prior = prior\n",
        "\n",
        "        self.children = []\n",
        "\n",
        "        self.visit_count = visit_count\n",
        "        self.value_sum = 0\n",
        "\n",
        "    def is_fully_expanded(self):\n",
        "        return len(self.children) > 0\n",
        "\n",
        "    def select(self):\n",
        "        best_child = None\n",
        "        best_ucb = -np.inf\n",
        "\n",
        "        for child in self.children:\n",
        "            ucb = self.get_ucb(child)\n",
        "            if ucb > best_ucb:\n",
        "                best_child = child\n",
        "                best_ucb = ucb\n",
        "\n",
        "        return best_child\n",
        "\n",
        "    def get_ucb(self, child):\n",
        "        if child.visit_count == 0:\n",
        "            q_value = 0\n",
        "        else:\n",
        "            q_value = 1 - ((child.value_sum / child.visit_count) + 1) / 2\n",
        "        return q_value + self.args['C'] * (math.sqrt(self.visit_count) / (child.visit_count + 1)) * child.prior\n",
        "\n",
        "    def expand(self, policy):\n",
        "        for action, prob in enumerate(policy):\n",
        "            if prob > 0:\n",
        "                child_state = self.state.copy()\n",
        "                child_state = self.game.get_next_state(child_state, action, 1)\n",
        "                child_state = self.game.change_perspective(child_state, player=-1)\n",
        "\n",
        "                child = Node(self.game, self.args, child_state, self, action, prob)\n",
        "                self.children.append(child)\n",
        "\n",
        "        return child\n",
        "\n",
        "    def backpropagate(self, value):\n",
        "        self.value_sum += value\n",
        "        self.visit_count += 1\n",
        "\n",
        "        value = self.game.get_opponent_value(value)\n",
        "        if self.parent is not None:\n",
        "            self.parent.backpropagate(value)\n",
        "\n",
        "\n",
        "class MCTS:\n",
        "    def __init__(self, game, args, model):\n",
        "        self.game = game\n",
        "        self.args = args\n",
        "        self.model = model\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def search(self, state):\n",
        "        root = Node(self.game, self.args, state, visit_count=1)\n",
        "\n",
        "        policy, _ = self.model(\n",
        "            torch.tensor(self.game.get_encoded_state(state), device=self.model.device).unsqueeze(0)\n",
        "        )\n",
        "        policy = torch.softmax(policy, axis=1).squeeze(0).cpu().numpy()\n",
        "        policy = (1 - self.args['dirichlet_epsilon']) * policy + self.args['dirichlet_epsilon'] \\\n",
        "            * np.random.dirichlet([self.args['dirichlet_alpha']] * self.game.action_size)\n",
        "\n",
        "        valid_moves = self.game.get_valid_moves(state)\n",
        "        policy *= valid_moves\n",
        "        policy /= np.sum(policy)\n",
        "        root.expand(policy)\n",
        "\n",
        "        for search in range(self.args['num_searches']):\n",
        "            node = root\n",
        "\n",
        "            while node.is_fully_expanded():\n",
        "                node = node.select()\n",
        "\n",
        "            value, is_terminal = self.game.get_value_and_terminated(node.state, node.action_taken)\n",
        "            value = self.game.get_opponent_value(value)\n",
        "\n",
        "            if not is_terminal:\n",
        "                policy, value = self.model(\n",
        "                    torch.tensor(self.game.get_encoded_state(node.state), device=self.model.device).unsqueeze(0)\n",
        "                )\n",
        "                policy = torch.softmax(policy, axis=1).squeeze(0).cpu().numpy()\n",
        "                valid_moves = self.game.get_valid_moves(node.state)\n",
        "                policy *= valid_moves\n",
        "                policy /= np.sum(policy)\n",
        "\n",
        "                value = value.item()\n",
        "\n",
        "                node.expand(policy)\n",
        "\n",
        "            node.backpropagate(value)\n",
        "\n",
        "\n",
        "        action_probs = np.zeros(self.game.action_size)\n",
        "        for child in root.children:\n",
        "            action_probs[child.action_taken] = child.visit_count\n",
        "        action_probs /= np.sum(action_probs)\n",
        "        return action_probs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3b28ab8",
      "metadata": {
        "id": "a3b28ab8"
      },
      "outputs": [],
      "source": [
        "class AlphaZero:\n",
        "    def __init__(self, model, optimizer, game, args):\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        self.game = game\n",
        "        self.args = args\n",
        "        self.mcts = MCTS(game, args, model)\n",
        "\n",
        "    def selfPlay(self):\n",
        "        memory = []\n",
        "        player = 1\n",
        "        state = self.game.get_initial_state()\n",
        "\n",
        "        while True:\n",
        "            neutral_state = self.game.change_perspective(state, player)\n",
        "            action_probs = self.mcts.search(neutral_state)\n",
        "\n",
        "            memory.append((neutral_state, action_probs, player))\n",
        "\n",
        "            temperature_action_probs = action_probs ** (1 / self.args['temperature'])\n",
        "            action = np.random.choice(self.game.action_size, p=temperature_action_probs/np.sum(temperature_action_probs)) # Divide temperature_action_probs with its sum in case of an error\n",
        "\n",
        "            state = self.game.get_next_state(state, action, player)\n",
        "\n",
        "            value, is_terminal = self.game.get_value_and_terminated(state, action)\n",
        "\n",
        "            if is_terminal:\n",
        "                returnMemory = []\n",
        "                for hist_neutral_state, hist_action_probs, hist_player in memory:\n",
        "                    hist_outcome = value if hist_player == player else self.game.get_opponent_value(value)\n",
        "                    returnMemory.append((\n",
        "                        self.game.get_encoded_state(hist_neutral_state),\n",
        "                        hist_action_probs,\n",
        "                        hist_outcome\n",
        "                    ))\n",
        "                return returnMemory\n",
        "\n",
        "            player = self.game.get_opponent(player)\n",
        "\n",
        "    def train(self, memory):\n",
        "        random.shuffle(memory)\n",
        "        for batchIdx in range(0, len(memory), self.args['batch_size']):\n",
        "            sample = memory[batchIdx:min(len(memory) - 1, batchIdx + self.args['batch_size'])] # Change to memory[batchIdx:batchIdx+self.args['batch_size']] in case of an error\n",
        "            state, policy_targets, value_targets = zip(*sample)\n",
        "\n",
        "            state, policy_targets, value_targets = np.array(state), np.array(policy_targets), np.array(value_targets).reshape(-1, 1)\n",
        "\n",
        "            state = torch.tensor(state, dtype=torch.float32, device=self.model.device)\n",
        "            policy_targets = torch.tensor(policy_targets, dtype=torch.float32, device=self.model.device)\n",
        "            value_targets = torch.tensor(value_targets, dtype=torch.float32, device=self.model.device)\n",
        "\n",
        "            out_policy, out_value = self.model(state)\n",
        "\n",
        "            policy_loss = F.cross_entropy(out_policy, policy_targets)\n",
        "            value_loss = F.mse_loss(out_value, value_targets)\n",
        "            loss = policy_loss + value_loss\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "    def learn(self):\n",
        "        for iteration in range(self.args['num_iterations']):\n",
        "            memory = []\n",
        "\n",
        "            self.model.eval()\n",
        "            for selfPlay_iteration in trange(self.args['num_selfPlay_iterations']):\n",
        "                memory += self.selfPlay()\n",
        "\n",
        "            self.model.train()\n",
        "            for epoch in trange(self.args['num_epochs']):\n",
        "                self.train(memory)\n",
        "\n",
        "            torch.save(self.model.state_dict(), f\"model_{iteration}_{self.game}.pt\")\n",
        "            torch.save(self.optimizer.state_dict(), f\"optimizer_{iteration}_{self.game}.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca3248e3",
      "metadata": {
        "id": "ca3248e3"
      },
      "source": [
        "### Optimized and parallel (even if without threads) version of MCTS and Alpha Zero"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e997f3ea",
      "metadata": {
        "id": "e997f3ea"
      },
      "outputs": [],
      "source": [
        "class MCTSParallel:\n",
        "    def __init__(self, game, args, model):\n",
        "        self.game = game\n",
        "        self.args = args\n",
        "        self.model = model\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def search(self, states, spGames):\n",
        "        policy, _ = self.model(\n",
        "            torch.tensor(self.game.get_encoded_state(states), device=self.model.device)\n",
        "        )\n",
        "        policy = torch.softmax(policy, axis=1).cpu().numpy()\n",
        "        policy = (1 - self.args['dirichlet_epsilon']) * policy + self.args['dirichlet_epsilon'] \\\n",
        "            * np.random.dirichlet([self.args['dirichlet_alpha']] * self.game.action_size, size=policy.shape[0])\n",
        "\n",
        "        for i, spg in enumerate(spGames):\n",
        "            spg_policy = policy[i]\n",
        "            valid_moves = self.game.get_valid_moves(states[i])\n",
        "            spg_policy *= valid_moves\n",
        "            spg_policy /= np.sum(spg_policy)\n",
        "\n",
        "            spg.root = Node(self.game, self.args, states[i], visit_count=1)\n",
        "            spg.root.expand(spg_policy)\n",
        "\n",
        "        for search in range(self.args['num_searches']):\n",
        "            for spg in spGames:\n",
        "\n",
        "                spg.node = None\n",
        "                node = spg.root\n",
        "\n",
        "                while node.is_fully_expanded():\n",
        "                    node = node.select()\n",
        "\n",
        "                value, is_terminal = self.game.get_value_and_terminated(node.state, node.action_taken)\n",
        "                value = self.game.get_opponent_value(value)\n",
        "\n",
        "                if is_terminal:\n",
        "                    node.backpropagate(value)\n",
        "\n",
        "                else:\n",
        "                    spg.node = node\n",
        "\n",
        "            expandable_spGames = [mappingIdx for mappingIdx in range(len(spGames)) if spGames[mappingIdx].node is not None]\n",
        "\n",
        "            if len(expandable_spGames) > 0:\n",
        "                states = np.stack([spGames[mappingIdx].node.state for mappingIdx in expandable_spGames])\n",
        "\n",
        "                policy, value = self.model(\n",
        "                    torch.tensor(self.game.get_encoded_state(states), device=self.model.device)\n",
        "                )\n",
        "                policy = torch.softmax(policy, axis=1).cpu().numpy()\n",
        "                value = value.cpu().numpy()\n",
        "\n",
        "            for i, mappingIdx in enumerate(expandable_spGames):\n",
        "                node = spGames[mappingIdx].node\n",
        "                spg_policy, spg_value = policy[i], value[i]\n",
        "\n",
        "                valid_moves = self.game.get_valid_moves(node.state)\n",
        "                spg_policy *= valid_moves\n",
        "                spg_policy /= np.sum(spg_policy)\n",
        "\n",
        "                node.expand(spg_policy)\n",
        "                node.backpropagate(spg_value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d0a5a7d",
      "metadata": {
        "id": "7d0a5a7d"
      },
      "outputs": [],
      "source": [
        "class AlphaZeroParallel:\n",
        "    def __init__(self, model, optimizer, game, args):\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        self.game = game\n",
        "        self.args = args\n",
        "        self.mcts = MCTSParallel(game, args, model)\n",
        "\n",
        "    def selfPlay(self):\n",
        "        return_memory = []\n",
        "        player = 1\n",
        "        spGames = [SPG(self.game) for spg in range(self.args['num_parallel_games'])]\n",
        "\n",
        "        while len(spGames) > 0:\n",
        "            print(len(spGames))\n",
        "            states = np.stack([spg.state for spg in spGames])\n",
        "            neutral_states = self.game.change_perspective(states, player)\n",
        "\n",
        "            self.mcts.search(neutral_states, spGames)\n",
        "\n",
        "            for i in range(len(spGames))[::-1]:\n",
        "                print(i)\n",
        "\n",
        "                spg = spGames[i]\n",
        "\n",
        "                action_probs = np.zeros(self.game.action_size)\n",
        "                for child in spg.root.children:\n",
        "                    action_probs[child.action_taken] = child.visit_count\n",
        "                action_probs /= np.sum(action_probs)\n",
        "\n",
        "                spg.memory.append((spg.root.state, action_probs, player))\n",
        "\n",
        "                temperature_action_probs = action_probs ** (1 / self.args['temperature'])\n",
        "                action = np.random.choice(self.game.action_size, p=(temperature_action_probs/np.sum(temperature_action_probs))) # Divide temperature_action_probs with its sum in case of an error\n",
        "\n",
        "                spg.state = self.game.get_next_state(spg.state, action, player)\n",
        "\n",
        "                value, is_terminal = self.game.get_value_and_terminated(spg.state, action)\n",
        "\n",
        "                if is_terminal:\n",
        "                    for hist_neutral_state, hist_action_probs, hist_player in spg.memory:\n",
        "                        hist_outcome = value if hist_player == player else self.game.get_opponent_value(value)\n",
        "                        return_memory.append((\n",
        "                            self.game.get_encoded_state(hist_neutral_state),\n",
        "                            hist_action_probs,\n",
        "                            hist_outcome\n",
        "                        ))\n",
        "                    del spGames[i]\n",
        "\n",
        "            player = self.game.get_opponent(player)\n",
        "\n",
        "        return return_memory\n",
        "\n",
        "    def train(self, memory):\n",
        "        random.shuffle(memory)\n",
        "        for batchIdx in range(0, len(memory), self.args['batch_size']):\n",
        "            sample = memory[batchIdx:min(len(memory) - 1, batchIdx + self.args['batch_size'])] # Change to memory[batchIdx:batchIdx+self.args['batch_size']] in case of an error\n",
        "            state, policy_targets, value_targets = zip(*sample)\n",
        "\n",
        "            state, policy_targets, value_targets = np.array(state), np.array(policy_targets), np.array(value_targets).reshape(-1, 1)\n",
        "\n",
        "            state = torch.tensor(state, dtype=torch.float32, device=self.model.device)\n",
        "            policy_targets = torch.tensor(policy_targets, dtype=torch.float32, device=self.model.device)\n",
        "            value_targets = torch.tensor(value_targets, dtype=torch.float32, device=self.model.device)\n",
        "\n",
        "            out_policy, out_value = self.model(state)\n",
        "\n",
        "            policy_loss = F.cross_entropy(out_policy, policy_targets)\n",
        "            value_loss = F.mse_loss(out_value, value_targets)\n",
        "            loss = policy_loss + value_loss\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "    def learn(self):\n",
        "        for iteration in range(self.args['num_iterations']):\n",
        "            memory = []\n",
        "\n",
        "            print(\"Evaluation\")\n",
        "            self.model.eval()\n",
        "            print(\"Self play\")\n",
        "            i = 0\n",
        "            for selfPlay_iteration in trange(self.args['num_selfPlay_iterations'] // self.args['num_parallel_games']):\n",
        "                print(\"Playing game\", i)\n",
        "                memory += self.selfPlay()\n",
        "                i += 1\n",
        "\n",
        "            print(\"Train\")\n",
        "            i = 0\n",
        "            self.model.train()\n",
        "            for epoch in trange(self.args['num_epochs']):\n",
        "                print(\"Training epoch\", i)\n",
        "                self.train(memory)\n",
        "                i += 1\n",
        "\n",
        "            torch.save(self.model.state_dict(), f\"model_{iteration}_{self.game}.pt\")\n",
        "            torch.save(self.optimizer.state_dict(), f\"optimizer_{iteration}_{self.game}.pt\")\n",
        "\n",
        "class SPG:\n",
        "    def __init__(self, game):\n",
        "        self.state = game.get_initial_state()\n",
        "        self.memory = []\n",
        "        self.root = None\n",
        "        self.node = None"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a26531e0",
      "metadata": {
        "id": "a26531e0"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "24bd91ef",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "24bd91ef",
        "outputId": "c3662876-fb06-465d-ccb0-aa501126dc1a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'ConnectFour' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-03a9686614ce>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConnectFour\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mResNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'ConnectFour' is not defined"
          ]
        }
      ],
      "source": [
        "game = ConnectFour()\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = ResNet(game, 9, 128, device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
        "\n",
        "args = {\n",
        "    'C': 2,\n",
        "    'num_searches': 600,\n",
        "    'num_iterations': 8,\n",
        "    'num_selfPlay_iterations': 500,\n",
        "    'num_parallel_games': 100,\n",
        "    'num_epochs': 4,\n",
        "    'batch_size': 128,\n",
        "    'temperature': 1.25,\n",
        "    'dirichlet_epsilon': 0.25,\n",
        "    'dirichlet_alpha': 0.3\n",
        "}\n",
        "\n",
        "alphaZero = AlphaZero(model, optimizer, game, args)\n",
        "alphaZero.learn()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2da25d96",
      "metadata": {
        "id": "2da25d96"
      },
      "source": [
        "### Test to see how it plays\n",
        "\n",
        "To make it play we only need the model (ResNet), the file produced by the training (.pt file) and the MCTS algorithm (standard version)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c470145",
      "metadata": {
        "id": "7c470145",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "game = ConnectFour()\n",
        "player = 1\n",
        "\n",
        "args = {\n",
        "    'C': 2,\n",
        "    'num_searches': 600,\n",
        "    'dirichlet_epsilon': 0.,\n",
        "    'dirichlet_alpha': 0.3\n",
        "}\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = ResNet(game, 9, 128, device)\n",
        "model.load_state_dict(torch.load(\"model_7_ConnectFour.pt\", map_location=device))\n",
        "model.eval()\n",
        "\n",
        "mcts = MCTS(game, args, model)\n",
        "\n",
        "state = game.get_initial_state()\n",
        "\n",
        "\n",
        "while True:\n",
        "    print(state)\n",
        "\n",
        "    if player == 1:\n",
        "        valid_moves = game.get_valid_moves(state)\n",
        "        print(\"valid_moves\", [i for i in range(game.action_size) if valid_moves[i] == 1])\n",
        "        action = int(input(f\"{player}:\"))\n",
        "\n",
        "        if valid_moves[action] == 0:\n",
        "            print(\"action not valid\")\n",
        "            continue\n",
        "\n",
        "    else:\n",
        "        neutral_state = game.change_perspective(state, player)\n",
        "        mcts_probs = mcts.search(neutral_state)\n",
        "        action = np.argmax(mcts_probs)\n",
        "\n",
        "    state = game.get_next_state(state, action, player)\n",
        "\n",
        "    value, is_terminal = game.get_value_and_terminated(state, action)\n",
        "\n",
        "    if is_terminal:\n",
        "        print(state)\n",
        "        if value == 1:\n",
        "            print(player, \"won\")\n",
        "        else:\n",
        "            print(\"draw\")\n",
        "        break\n",
        "\n",
        "    player = game.get_opponent(player)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "antelligence",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}