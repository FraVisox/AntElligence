{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6a2b6a16",
      "metadata": {
        "id": "6a2b6a16"
      },
      "source": [
        "# Implementation of Alpha Zero for our game\n",
        "This follows the ideas of idea.md"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "2d090f5e",
      "metadata": {
        "id": "2d090f5e",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from tqdm.notebook import trange\n",
        "\n",
        "import random\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc1043ac",
      "metadata": {
        "id": "fc1043ac"
      },
      "source": [
        "### OUR GAME"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "gU_nu7Ecwutr",
      "metadata": {
        "id": "gU_nu7Ecwutr"
      },
      "outputs": [],
      "source": [
        "# compile with  g++ -shared -o engine.dll *.cpp -static -static-libgcc -static-libstdc++ -I. -O2 -Wall -DBUILDING_DLL\n",
        "# for linux compile with g++ -fPIC -shared -o engine.dll *.cpp\n",
        "\n",
        "import ctypes\n",
        "import os\n",
        "\n",
        "# This represents the return types of the functions\n",
        "class ReturnTypes:\n",
        "    OK = 0\n",
        "    ERROR = 1\n",
        "    GAME_OVER_DRAW = 2\n",
        "    GAME_OVER_WHITE_WINS = 3\n",
        "    GAME_OVER_BLACK_WINS = 4\n",
        "    INVALID_ARGUMENT = 5\n",
        "    INVALID_GAME_NOT_STARTED = 6\n",
        "\n",
        "class EngineDLL():\n",
        "    def __init__(self,gamestring: str = \"\"):\n",
        "        #TODO: place engine.dll in content/ directory\n",
        "        path = os.path.join(os.getcwd(), \"engine.dll\")\n",
        "        self.dll = ctypes.CDLL(path)\n",
        "        self._setup_functions()\n",
        "        self.start_game(gamestring)\n",
        "\n",
        "    def _setup_functions(self):\n",
        "        # Set up game functions\n",
        "        self.dll.startGame.argtypes = [ctypes.c_char_p]\n",
        "        self.dll.startGame.restype = ctypes.c_int\n",
        "\n",
        "        self.dll.playMove.argtypes = [ctypes.c_char_p]\n",
        "        self.dll.playMove.restype = ctypes.c_int\n",
        "\n",
        "        self.dll.validMoves.restype = ctypes.c_char_p\n",
        "\n",
        "        self.dll.getBoard.restype = ctypes.c_char_p\n",
        "\n",
        "        self.dll.undo.argtypes = [ctypes.c_int]\n",
        "\n",
        "        self.dll.getTurn.restype = ctypes.c_int\n",
        "\n",
        "        self.dll.oracleEval.restype=ctypes.c_float\n",
        "\n",
        "    def current_player_turn(self)->int:\n",
        "        return self.dll.getTurn()\n",
        "\n",
        "    def valid_moves(self)->str:\n",
        "        return self.get_valid_moves()\n",
        "\n",
        "    def play(self,move_string: str)-> None:\n",
        "        self.play_move(move_string)\n",
        "\n",
        "    def start_game(self, game_string):\n",
        "        encoded_string = game_string.encode(\"utf-8\")\n",
        "        return self.dll.startGame(encoded_string) == ReturnTypes.OK\n",
        "\n",
        "    def play_move(self, move_string):\n",
        "        encoded_string = move_string.encode(\"utf-8\")\n",
        "        return self.dll.playMove(encoded_string)\n",
        "\n",
        "    def current_player_turn(self)->int:\n",
        "        return 1\n",
        "\n",
        "    def get_valid_moves(self):\n",
        "        try:\n",
        "            result = self.dll.validMoves()\n",
        "            if result:\n",
        "                raw_bytes = ctypes.string_at(result)\n",
        "                try:\n",
        "                    decoded = raw_bytes.decode(\"utf-8\")\n",
        "                    return decoded\n",
        "                except UnicodeDecodeError as e:\n",
        "                    return raw_bytes.decode('ascii', errors='replace')\n",
        "        except Exception:\n",
        "            return \"\"\n",
        "\n",
        "    def get_board(self):\n",
        "        result = self.dll.getBoard()\n",
        "        if result:\n",
        "            try:\n",
        "                return ctypes.string_at(result).decode(\"utf-8\")\n",
        "            except UnicodeDecodeError:\n",
        "                return ctypes.string_at(result).decode('ascii', errors='replace')\n",
        "        return \"\"\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Z4oPS1j1uv70"
      },
      "outputs": [],
      "source": [
        "## ENGINE\n",
        "\n",
        "class Engine():\n",
        "\n",
        "  def __init__(self):\n",
        "    # Create a singleton instance\n",
        "    self.CPPInterface = EngineDLL()\n",
        "\n",
        "  # Init the game\n",
        "  def newgame(self, arguments: list[str]) -> None:\n",
        "    self.CPPInterface.start_game(\" \".join(arguments))\n",
        "\n",
        "  ##TODO: define a state and a board in a convenient way\n",
        "\n",
        "  def get_initial_state(self):\n",
        "    self.newgame(\"Base+MLP\")\n",
        "    return self.CPPInterface.get_board()\n",
        "\n",
        "  #TODO: how are these passed?\n",
        "  def get_valid_moves(self, state):\n",
        "    return self.CPPInterface.get_valid_moves(state) #TODO: does it make sense to pass the state here?\n",
        "\n",
        "  def get_next_state(self, state, action, player):\n",
        "    self.CPPInterface.play_move(state, action) #Player is useless\n",
        "    return self.CPPInterface.get_board()\n",
        "\n",
        "  def check_win(self, state): #TODO: check if this action gets to a winning state\n",
        "    return self.CPPInterface.check_win(state)\n",
        "\n",
        "  def get_value_and_terminated(self, state): #TODO: fix, maybe it's finished and we just lost\n",
        "        if self.check_win(state):\n",
        "            return 1, True\n",
        "        if np.sum(self.get_valid_moves(state)) == 0:\n",
        "            return 0, True\n",
        "        return 0, False\n",
        "\n",
        "  def get_turn(self):\n",
        "    return self.CPPInterface.current_player_turn()\n",
        "\n",
        "  def get_opponent(self, player):\n",
        "        return -player\n",
        "\n",
        "  def get_opponent_value(self, value):\n",
        "      return -value\n",
        "\n",
        "  def change_perspective(self, state, player):\n",
        "      return state * player\n",
        "\n",
        "  def get_encoded_state(self, state):\n",
        "      encoded_state = np.stack(\n",
        "          (state == -1, state == 0, state == 1)\n",
        "      ).astype(np.float32)\n",
        "\n",
        "      if len(state.shape) == 3:\n",
        "          encoded_state = np.swapaxes(encoded_state, 0, 1)\n",
        "\n",
        "      return encoded_state\n",
        "\n",
        "  def get_graph_representation(self, state):\n",
        "    \"\"\"\n",
        "    Convert the adjacency matrix state representation to graph format.\n",
        "\n",
        "    Args:\n",
        "        state: The adjacency matrix representation\n",
        "\n",
        "    Returns:\n",
        "        node_features: Features for each node/piece [num_nodes, feature_dim]\n",
        "        edge_index: Graph connectivity in COO format [2, num_edges]\n",
        "    \"\"\"\n",
        "    num_pieces = state.shape[0]  # Number of rows = number of pieces\n",
        "\n",
        "    # Initialize node features\n",
        "    # Features could include: piece type, player ownership, etc.\n",
        "    node_features = []\n",
        "    for i in range(num_pieces):\n",
        "        # Get piece type and owner from your game state\n",
        "        piece_type = self.get_piece_type(state, i)\n",
        "        piece_owner = self.get_piece_owner(state, i)\n",
        "\n",
        "        # Create one-hot encodings or other suitable features. TODO\n",
        "        features = [...]  # Construct appropriate features\n",
        "        node_features.append(features)\n",
        "\n",
        "    # Construct edge list from adjacency information\n",
        "    edge_src = []\n",
        "    edge_dst = []\n",
        "\n",
        "    # For each piece (row in the adjacency matrix)\n",
        "    for i in range(num_pieces):\n",
        "        # For each connection direction (column)\n",
        "        for j in range(state.shape[1]):\n",
        "            connected_piece = state[i, j]\n",
        "            if connected_piece >= 0:  # If there's a connection (not -1 or empty)\n",
        "                edge_src.append(i)\n",
        "                edge_dst.append(connected_piece)\n",
        "\n",
        "    edge_index = [edge_src, edge_dst]\n",
        "\n",
        "    return np.array(node_features), np.array(edge_index)"
      ],
      "id": "Z4oPS1j1uv70"
    },
    {
      "cell_type": "markdown",
      "id": "a1761699",
      "metadata": {
        "id": "a1761699"
      },
      "source": [
        "### Neural Network Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch_geometric"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMArSHv0PRS_",
        "outputId": "35d2a45c-ef14-4267-8bea-1eb436f6aa5d"
      },
      "id": "OMArSHv0PRS_",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.11.15)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2025.3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (6.3.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.18.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch_geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2025.1.31)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "02e5b58b"
      },
      "outputs": [],
      "source": [
        "# Neural network used to output two things: a policy (= probability that from here, doing a certain move gives the winning) and a value (= evaluation of current state)\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv, global_mean_pool\n",
        "\n",
        "class HiveGNN(nn.Module):\n",
        "    def __init__(self, node_features, hidden_dim, device):\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "\n",
        "        # Initial graph convolution\n",
        "        self.conv1 = GCNConv(node_features, hidden_dim)\n",
        "\n",
        "        # Several graph convolution layers (like your ResBlocks)\n",
        "        self.conv_layers = nn.ModuleList([\n",
        "            GCNConv(hidden_dim, hidden_dim) for _ in range(9)  # Similar to your 9 ResBlocks\n",
        "        ])\n",
        "\n",
        "        # Policy head - now operates on node embeddings\n",
        "        self.policy_mlp = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Final policy output will be generated dynamically based on valid moves\n",
        "\n",
        "        # Value head\n",
        "        self.value_head = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim // 2, 1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "        self.to(device)\n",
        "\n",
        "    def forward(self, x, edge_index, batch=None):\n",
        "        # Initial convolution\n",
        "        h = F.relu(self.conv1(x, edge_index))\n",
        "\n",
        "        # Graph convolution layers\n",
        "        for conv in self.conv_layers:\n",
        "            # Optional: add residual connections\n",
        "            residual = h\n",
        "            h = F.relu(conv(h, edge_index))\n",
        "            h = h + residual  # Residual connection\n",
        "\n",
        "        # Node embeddings are now in h\n",
        "\n",
        "        # For value: global pooling then MLP\n",
        "        if batch is not None:\n",
        "            pooled = global_mean_pool(h, batch)  # [batch_size, hidden_dim]\n",
        "        else:\n",
        "            pooled = h.mean(dim=0, keepdim=True)  # [1, hidden_dim]\n",
        "\n",
        "        value = self.value_head(pooled)\n",
        "\n",
        "        # For policy: get embeddings for each node\n",
        "        node_policy_features = self.policy_mlp(h)  # [num_nodes, hidden_dim//2]\n",
        "\n",
        "        # node_policy_features can be used to dynamically compute action probabilities\n",
        "        # for the valid moves in the current state\n",
        "\n",
        "        return node_policy_features, value\n"
      ],
      "id": "02e5b58b"
    },
    {
      "cell_type": "markdown",
      "id": "25817615",
      "metadata": {
        "id": "25817615"
      },
      "source": [
        "### Basic MCTS and Alpha Zero"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "21866526",
      "metadata": {
        "id": "21866526"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "# A node of the Tree: modified to handle variable action spaces\n",
        "class Node:\n",
        "    def __init__(self, game, args, state, parent=None, action_taken=None, prior=0, visit_count=0):\n",
        "        self.game = game\n",
        "        self.args = args\n",
        "        self.state = state\n",
        "        self.parent = parent\n",
        "        self.action_taken = action_taken  # Now this will be a tuple or object representing the move\n",
        "        self.prior = prior\n",
        "\n",
        "        self.children = []\n",
        "        self.valid_actions = []  # Store valid actions for this node\n",
        "\n",
        "        self.visit_count = visit_count\n",
        "        self.value_sum = 0\n",
        "\n",
        "    # -------------------------------------- SELECT PHASE -------------------------------------------------------\n",
        "\n",
        "    def is_fully_expanded(self):\n",
        "        return len(self.children) > 0\n",
        "\n",
        "    def select(self):\n",
        "        best_child = None\n",
        "        best_ucb = -np.inf\n",
        "\n",
        "        for child in self.children:\n",
        "            ucb = self.get_ucb(child)\n",
        "            if ucb > best_ucb:\n",
        "                best_child = child\n",
        "                best_ucb = ucb\n",
        "\n",
        "        return best_child\n",
        "\n",
        "    def get_ucb(self, child):\n",
        "        if child.visit_count == 0:\n",
        "            q_value = 0\n",
        "        else:\n",
        "            q_value = 1 - ((child.value_sum / child.visit_count) + 1) / 2\n",
        "        return q_value + self.args['C'] * (math.sqrt(self.visit_count) / (child.visit_count + 1)) * child.prior\n",
        "\n",
        "    # -------------------------------------- EXPANSION PHASE -------------------------------------------------------\n",
        "\n",
        "    # Modified to handle action probabilities from graph neural network\n",
        "    def expand(self, action_probs):\n",
        "        # action_probs should be a dictionary mapping actions to probabilities\n",
        "        self.valid_actions = list(action_probs.keys())\n",
        "\n",
        "        for action, prob in action_probs.items():\n",
        "            if prob > 0:\n",
        "                child_state = self.state.copy()\n",
        "                child_state = self.game.get_next_state(child_state, action, 1)\n",
        "                child_state = self.game.change_perspective(child_state, player=-1)\n",
        "\n",
        "                child = Node(self.game, self.args, child_state, self, action, prob)\n",
        "                self.children.append(child)\n",
        "\n",
        "        return self.children[0] if self.children else None\n",
        "\n",
        "    # -------------------------------------- BACKPROPAGATION PHASE -------------------------------------------------------\n",
        "\n",
        "    def backpropagate(self, value):\n",
        "        self.value_sum += value\n",
        "        self.visit_count += 1\n",
        "\n",
        "        value = self.game.get_opponent_value(value)\n",
        "        if self.parent is not None:\n",
        "            self.parent.backpropagate(value)\n",
        "\n",
        "\n",
        "# MCTS class, modified to work with graph neural networks\n",
        "class MCTS:\n",
        "    def __init__(self, game, args, model):\n",
        "        self.game = game\n",
        "        self.args = args\n",
        "        self.model = model\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def search(self, state, turn=1):\n",
        "        # Initialize the root\n",
        "        root = Node(self.game, self.args, state, visit_count=1)\n",
        "\n",
        "        # Get the graph representation of the state\n",
        "        node_features, edge_index = self.game.get_graph_representation(state)\n",
        "\n",
        "        # Convert to PyTorch tensors\n",
        "        node_features = torch.tensor(node_features, device=self.model.device).float()\n",
        "        edge_index = torch.tensor(edge_index, device=self.model.device).long()\n",
        "\n",
        "        # Get node embeddings and value from the model\n",
        "        node_embeddings, value = self.model(node_features, edge_index)\n",
        "\n",
        "        # Get valid moves for this state\n",
        "        valid_moves = self.game.get_valid_moves(state)\n",
        "\n",
        "        # Compute action probabilities from node embeddings\n",
        "        action_probs = self.compute_action_probabilities(node_embeddings, valid_moves, state)\n",
        "\n",
        "        # Apply Dirichlet noise for exploration (only for root)\n",
        "        if turn <= self.args['dirichlet_turn']:\n",
        "            # Adapt Dirichlet noise for variable action space\n",
        "            actions = list(action_probs.keys())\n",
        "            probs = np.array(list(action_probs.values()))\n",
        "            noise = np.random.dirichlet([self.args['dirichlet_alpha']] * len(probs))\n",
        "\n",
        "            for i, action in enumerate(actions):\n",
        "                action_probs[action] = (1 - self.args['dirichlet_epsilon']) * action_probs[action] + \\\n",
        "                                      self.args['dirichlet_epsilon'] * noise[i]\n",
        "\n",
        "            # Normalize\n",
        "            total = sum(action_probs.values())\n",
        "            action_probs = {a: p/total for a, p in action_probs.items()}\n",
        "\n",
        "        # Expand the root\n",
        "        root.expand(action_probs)\n",
        "\n",
        "        # Do a determined number of searches\n",
        "        for search in range(self.args['num_searches']):\n",
        "            node = root\n",
        "\n",
        "            # Search a leaf\n",
        "            while node.is_fully_expanded():\n",
        "                node = node.select()\n",
        "\n",
        "            # See if this is terminal\n",
        "            value, is_terminal = self.game.get_value_and_terminated(node.state, node.action_taken)\n",
        "            value = self.game.get_opponent_value(value)\n",
        "\n",
        "            # If it is not terminal\n",
        "            if not is_terminal:\n",
        "                # Get graph representation\n",
        "                node_features, edge_index = self.game.get_graph_representation(node.state)\n",
        "\n",
        "                # Convert to PyTorch tensors\n",
        "                node_features = torch.tensor(node_features, device=self.model.device).float()\n",
        "                edge_index = torch.tensor(edge_index, device=self.model.device).long()\n",
        "\n",
        "                # Get node embeddings and value from the model\n",
        "                node_embeddings, value = self.model(node_features, edge_index)\n",
        "\n",
        "                # Get valid moves\n",
        "                valid_moves = self.game.get_valid_moves(node.state)\n",
        "\n",
        "                # Compute action probabilities\n",
        "                action_probs = self.compute_action_probabilities(node_embeddings, valid_moves, node.state)\n",
        "\n",
        "                value = value.item()\n",
        "\n",
        "                # Expand\n",
        "                node.expand(action_probs)\n",
        "\n",
        "            # Backpropagate\n",
        "            node.backpropagate(value)\n",
        "\n",
        "        # Create action probabilities for return\n",
        "        visit_counts = {}\n",
        "        for child in root.children:\n",
        "            visit_counts[child.action_taken] = child.visit_count\n",
        "\n",
        "        total_visits = sum(visit_counts.values())\n",
        "        action_probs = {action: count/total_visits for action, count in visit_counts.items()}\n",
        "\n",
        "        return action_probs\n",
        "\n",
        "\n",
        "    #todo: this is how it is suggested by Claude. Understand how to train it\n",
        "\n",
        "    def compute_action_probabilities(self, node_embeddings, valid_moves, state):\n",
        "    \"\"\"\n",
        "    Compute action probabilities from node embeddings and valid moves.\n",
        "\n",
        "    Args:\n",
        "        node_embeddings: Tensor of shape [num_nodes, embedding_dim]\n",
        "        valid_moves: List of valid moves\n",
        "        state: Current game state\n",
        "\n",
        "    Returns:\n",
        "        Dictionary mapping each valid move to its probability\n",
        "    \"\"\"\n",
        "    action_probs = {}\n",
        "\n",
        "    # Convert node_embeddings to numpy for easier handling if needed\n",
        "    embeddings = node_embeddings.cpu().numpy() if isinstance(node_embeddings, torch.Tensor) else node_embeddings\n",
        "\n",
        "    # We'll use a small MLP to score each move\n",
        "    move_scorer = nn.Sequential(\n",
        "        nn.Linear(embeddings.shape[1] * 2, 64),  # Concatenated source and target embeddings\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(64, 1)\n",
        "    ).to(node_embeddings.device)\n",
        "\n",
        "    # For each valid move\n",
        "    move_logits = []\n",
        "    move_list = []\n",
        "\n",
        "    for move in valid_moves:\n",
        "        move_list.append(move)\n",
        "\n",
        "        if isinstance(move, tuple) and len(move) == 2:\n",
        "            # This is a movement move: (source_piece_idx, destination_position)\n",
        "            source_idx = move[0]\n",
        "\n",
        "            # For the destination, we need to get its current embedding\n",
        "            # This could be an existing piece or a position on the board\n",
        "            if isinstance(move[1], int):  # If destination is another piece index\n",
        "                target_idx = move[1]\n",
        "                target_embedding = embeddings[target_idx]\n",
        "            else:  # If destination is a position description\n",
        "                # We need to synthesize an embedding for this position\n",
        "                # You might use adjacent pieces to create this embedding\n",
        "                adjacent_pieces = self.game.get_adjacent_pieces(move[1], state)\n",
        "                if adjacent_pieces:\n",
        "                    # Average the embeddings of adjacent pieces\n",
        "                    adjacent_embeddings = [embeddings[p] for p in adjacent_pieces]\n",
        "                    target_embedding = np.mean(adjacent_embeddings, axis=0)\n",
        "                else:\n",
        "                    # Fallback if no adjacent pieces\n",
        "                    target_embedding = np.zeros_like(embeddings[0])\n",
        "\n",
        "            # Get source piece embedding\n",
        "            source_embedding = embeddings[source_idx]\n",
        "\n",
        "            # Concatenate source and target embeddings\n",
        "            combined = torch.tensor(\n",
        "                np.concatenate([source_embedding, target_embedding]),\n",
        "                device=node_embeddings.device\n",
        "            ).float()\n",
        "\n",
        "            # Get score for this move\n",
        "            logit = move_scorer(combined).item()\n",
        "\n",
        "        else:\n",
        "            # This is a placement move: typically a tuple with piece type and position\n",
        "            # (or however you represent placement moves)\n",
        "\n",
        "            # For placement, we might use the embedding of the piece type\n",
        "            # or the average embedding of pieces already on the board\n",
        "            piece_type = move[0] if isinstance(move, tuple) else move\n",
        "\n",
        "            # Get pieces of same type if any exist\n",
        "            same_type_pieces = [i for i in range(len(embeddings))\n",
        "                               if self.game.get_piece_type(state, i) == piece_type]\n",
        "\n",
        "            if same_type_pieces:\n",
        "                # Average embeddings of same type pieces\n",
        "                piece_embedding = np.mean([embeddings[i] for i in same_type_pieces], axis=0)\n",
        "            else:\n",
        "                # If no pieces of this type yet, use default embedding\n",
        "                piece_embedding = np.zeros_like(embeddings[0])\n",
        "\n",
        "            # Get position embedding similarly to movement case\n",
        "            pos = move[1] if isinstance(move, tuple) else None\n",
        "            if pos:\n",
        "                adjacent_pieces = self.game.get_adjacent_pieces(pos, state)\n",
        "                if adjacent_pieces:\n",
        "                    adjacent_embeddings = [embeddings[p] for p in adjacent_pieces]\n",
        "                    pos_embedding = np.mean(adjacent_embeddings, axis=0)\n",
        "                else:\n",
        "                    pos_embedding = np.zeros_like(embeddings[0])\n",
        "            else:\n",
        "                pos_embedding = np.zeros_like(embeddings[0])\n",
        "\n",
        "            # Concatenate piece and position embeddings\n",
        "            combined = torch.tensor(\n",
        "                np.concatenate([piece_embedding, pos_embedding]),\n",
        "                device=node_embeddings.device\n",
        "            ).float()\n",
        "\n",
        "            # Get score for this move\n",
        "            logit = move_scorer(combined).item()\n",
        "\n",
        "        move_logits.append(logit)\n",
        "\n",
        "    # Apply softmax to get probabilities\n",
        "    if move_logits:\n",
        "        logits = np.array(move_logits)\n",
        "        probs = np.exp(logits - np.max(logits))  # Subtract max for numerical stability\n",
        "        probs = probs / np.sum(probs)\n",
        "\n",
        "        action_probs = {move: float(prob) for move, prob in zip(move_list, probs)}\n",
        "\n",
        "    return action_probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "a3b28ab8",
      "metadata": {
        "id": "a3b28ab8"
      },
      "outputs": [],
      "source": [
        "class AlphaZero:\n",
        "    def __init__(self, model, optimizer, game, args):\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        self.game = game\n",
        "        self.args = args\n",
        "        self.mcts = MCTS(game, args, model)\n",
        "\n",
        "    def selfPlay(self):\n",
        "        memory = []\n",
        "        player = 1\n",
        "        state = self.game.get_initial_state()\n",
        "\n",
        "        while True:\n",
        "            neutral_state = self.game.change_perspective(state, player)\n",
        "            action_probs = self.mcts.search(neutral_state)\n",
        "\n",
        "            memory.append((neutral_state, action_probs, player))\n",
        "\n",
        "            temperature_action_probs = action_probs ** (1 / self.args['temperature'])\n",
        "            action = np.random.choice(self.game.action_size, p=temperature_action_probs/np.sum(temperature_action_probs)) # Divide temperature_action_probs with its sum in case of an error\n",
        "\n",
        "            state = self.game.get_next_state(state, action, player)\n",
        "\n",
        "            value, is_terminal = self.game.get_value_and_terminated(state, action)\n",
        "\n",
        "            if is_terminal:\n",
        "                returnMemory = []\n",
        "                for hist_neutral_state, hist_action_probs, hist_player in memory:\n",
        "                    hist_outcome = value if hist_player == player else self.game.get_opponent_value(value)\n",
        "                    returnMemory.append((\n",
        "                        self.game.get_encoded_state(hist_neutral_state),\n",
        "                        hist_action_probs,\n",
        "                        hist_outcome\n",
        "                    ))\n",
        "                return returnMemory\n",
        "\n",
        "            player = self.game.get_opponent(player)\n",
        "\n",
        "    def train(self, memory):\n",
        "        random.shuffle(memory)\n",
        "        for batchIdx in range(0, len(memory), self.args['batch_size']):\n",
        "            sample = memory[batchIdx:min(len(memory) - 1, batchIdx + self.args['batch_size'])] # Change to memory[batchIdx:batchIdx+self.args['batch_size']] in case of an error\n",
        "            state, policy_targets, value_targets = zip(*sample)\n",
        "\n",
        "            state, policy_targets, value_targets = np.array(state), np.array(policy_targets), np.array(value_targets).reshape(-1, 1)\n",
        "\n",
        "            state = torch.tensor(state, dtype=torch.float32, device=self.model.device)\n",
        "            policy_targets = torch.tensor(policy_targets, dtype=torch.float32, device=self.model.device)\n",
        "            value_targets = torch.tensor(value_targets, dtype=torch.float32, device=self.model.device)\n",
        "\n",
        "            out_policy, out_value = self.model(state)\n",
        "\n",
        "            policy_loss = F.cross_entropy(out_policy, policy_targets)\n",
        "            value_loss = F.mse_loss(out_value, value_targets)\n",
        "            loss = policy_loss + value_loss\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "    def learn(self):\n",
        "        for iteration in range(self.args['num_iterations']):\n",
        "            memory = []\n",
        "\n",
        "            self.model.eval()\n",
        "            for selfPlay_iteration in trange(self.args['num_selfPlay_iterations']):\n",
        "                memory += self.selfPlay()\n",
        "\n",
        "            self.model.train()\n",
        "            for epoch in trange(self.args['num_epochs']):\n",
        "                self.train(memory)\n",
        "\n",
        "            torch.save(self.model.state_dict(), f\"model_{iteration}_{self.game}.pt\")\n",
        "            torch.save(self.optimizer.state_dict(), f\"optimizer_{iteration}_{self.game}.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca3248e3",
      "metadata": {
        "id": "ca3248e3"
      },
      "source": [
        "### Optimized and parallel (even if without threads) version of MCTS and Alpha Zero"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO"
      ],
      "metadata": {
        "id": "_L_IxytGRP6d"
      },
      "id": "_L_IxytGRP6d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "a26531e0",
      "metadata": {
        "id": "a26531e0"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "24bd91ef",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "24bd91ef",
        "outputId": "e49c4d24-975c-4a93-bcb0-b663da9bd6c9"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "/content/engine.dll: invalid ELF header",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-00dbba8be2bc>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEngine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHiveGNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-835fb020bb34>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# Create a singleton instance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCPPInterface\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEngineDLL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0;31m# Init the game\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-d3f84db3f455>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, gamestring)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m#TODO: understand how to make this shared dll available in colab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"engine.dll\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCDLL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_functions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_game\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgamestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/ctypes/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_dlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: /content/engine.dll: invalid ELF header"
          ]
        }
      ],
      "source": [
        "game = Engine()\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = HiveGNN(game, 9, 128, device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
        "\n",
        "args = {\n",
        "    'C': 2,\n",
        "    'num_searches': 600,\n",
        "    'num_iterations': 8,\n",
        "    'num_selfPlay_iterations': 500,\n",
        "    'num_parallel_games': 100,\n",
        "    'num_epochs': 4,\n",
        "    'batch_size': 128,\n",
        "    'temperature': 1.25,\n",
        "    'dirichlet_epsilon': 0.25,\n",
        "    'dirichlet_alpha': 0.3\n",
        "}\n",
        "\n",
        "alphaZero = AlphaZero(model, optimizer, game, args)\n",
        "alphaZero.learn()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2da25d96",
      "metadata": {
        "id": "2da25d96"
      },
      "source": [
        "### Test to see how it plays\n",
        "\n",
        "To make it play we only need the model (ResNet), the file produced by the training (.pt file) and the MCTS algorithm (standard version)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c470145",
      "metadata": {
        "id": "7c470145",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "game = ConnectFour()\n",
        "player = 1\n",
        "\n",
        "args = {\n",
        "    'C': 2,\n",
        "    'num_searches': 600,\n",
        "    'dirichlet_epsilon': 0.,\n",
        "    'dirichlet_alpha': 0.3\n",
        "}\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = ResNet(game, 9, 128, device)\n",
        "model.load_state_dict(torch.load(\"model_7_ConnectFour.pt\", map_location=device))\n",
        "model.eval()\n",
        "\n",
        "mcts = MCTS(game, args, model)\n",
        "\n",
        "state = game.get_initial_state()\n",
        "\n",
        "\n",
        "while True:\n",
        "    print(state)\n",
        "\n",
        "    if player == 1:\n",
        "        valid_moves = game.get_valid_moves(state)\n",
        "        print(\"valid_moves\", [i for i in range(game.action_size) if valid_moves[i] == 1])\n",
        "        action = int(input(f\"{player}:\"))\n",
        "\n",
        "        if valid_moves[action] == 0:\n",
        "            print(\"action not valid\")\n",
        "            continue\n",
        "\n",
        "    else:\n",
        "        neutral_state = game.change_perspective(state, player)\n",
        "        mcts_probs = mcts.search(neutral_state)\n",
        "        action = np.argmax(mcts_probs)\n",
        "\n",
        "    state = game.get_next_state(state, action, player)\n",
        "\n",
        "    value, is_terminal = game.get_value_and_terminated(state, action)\n",
        "\n",
        "    if is_terminal:\n",
        "        print(state)\n",
        "        if value == 1:\n",
        "            print(player, \"won\")\n",
        "        else:\n",
        "            print(\"draw\")\n",
        "        break\n",
        "\n",
        "    player = game.get_opponent(player)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "antelligence",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}